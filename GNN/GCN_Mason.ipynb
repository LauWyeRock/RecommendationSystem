{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import json\n",
    "from pathlib import Path as Data_Path\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import relevant ML libraries\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Embedding, ModuleList, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from torch_geometric.nn.conv import LGConv, GATConv, SAGEConv\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}; Torch-cuda version: {torch.version.cuda}; Torch Geometric version: {torch_geometric.__version__}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for reproducibility\n",
    "seed = 224\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/user_songs_filtered.csv')\n",
    "data = data.sort_values(by = 'Username')\n",
    "data = data.reset_index(drop=True)\n",
    "data_subset = data.loc[data.Username <= 'SierraWuff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "  def __init__(self, track_dict):\n",
    "    self.name = track_dict[\"track_name\"]\n",
    "    self.artist_name = track_dict[\"artist_name\"]\n",
    "    self.listeners = track_dict[\"listeners\"]\n",
    "    self.total_playcount = track_dict[\"total_playcount\"]\n",
    "    self.emotion1 = track_dict[\"emotion1\"]\n",
    "    self.emotion1_score = track_dict[\"emotion1_score\"]\n",
    "    self.emotion2 = track_dict[\"emotion2\"]\n",
    "    self.emotion2_score = track_dict[\"emotion2_score\"]\n",
    "    self.rms = track_dict[\"rms\"]\n",
    "    self.spectral_centroid\t= track_dict['spectral_centroid']\n",
    "    self.tempo = track_dict['tempo']\n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"Track called {self.name} by ({self.artist_name}) has emotions {self.emotion1} and {self.emotion2}.\"\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Track {self.name}\"\n",
    "\n",
    "  def __lt__(self, other):\n",
    "    return (self.name < other.name) and (self.total_playcount < other.total_playcount)\n",
    "\n",
    "  def __gt__(self, other):\n",
    "    return (self.name > other.name) and (self.total_playcount > other.total_playcount)\n",
    "  \n",
    "data_subset[['track_name', 'artist_name']].drop_duplicates().to_numpy()\n",
    "\n",
    "tracks = []\n",
    "unique_tracks = data_subset[['track_name', 'artist_name']].drop_duplicates().to_numpy()\n",
    "for track, artist in unique_tracks:\n",
    "    row_data = data_subset.loc[(data_subset['track_name'] == track) & (data_subset['artist_name'] == artist)].iloc[0]\n",
    "    track_data = row_data[['track_name', 'artist_name', 'listeners', 'total_playcount', 'emotion1', 'emotion1_score',\n",
    "                           'emotion2', 'emotion2_score', 'rms', 'spectral_centroid', 'tempo']].to_dict()\n",
    "    tracks += [Track(track_data)]\n",
    "\n",
    "class User:\n",
    "  def __init__(self, user_data, top_songs):\n",
    "\n",
    "    self.name = user_data['Username']\n",
    "    self.country = user_data['country']\n",
    "    self.track_count = int(user_data['track_count'])\n",
    "    self.total_playcount = 0\n",
    "    self.top_songs = {}\n",
    "    self.artists = []\n",
    "    for index, row in top_songs.iterrows():\n",
    "      rank = row['rank']\n",
    "      track_name = row['track_name']\n",
    "      artist_name = row['artist_name']\n",
    "      track = [obj for obj in tracks if (obj.name == track_name)&(obj.artist_name == artist_name)][0]\n",
    "      playcount = row['playcount']\n",
    "      self.top_songs[rank] = (track, playcount)\n",
    "      self.total_playcount += playcount\n",
    "      # or should i use a dict for artists\n",
    "      self.artists +=[artist_name]\n",
    "\n",
    "    self.top_songs =  {k: self.top_songs[k] for k in sorted(self.top_songs)}\n",
    "  def __str__(self):\n",
    "    return f\"User {self.name} with {len(self.top_songs)} top tracks loaded, total listen count is {self.total_playcount}.\"\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"User {self.name}\"\n",
    "  def __lt__(self, other):\n",
    "    return (self.name < other.name) and (self.total_playcount < other.total_playcount)\n",
    "\n",
    "  def __gt__(self, other):\n",
    "    return (self.name > other.name) and (self.total_playcount > other.total_playcount)\n",
    "\n",
    "unique_users = data_subset.Username.unique()\n",
    "users = []\n",
    "for user in unique_users:\n",
    "    user_data = data_subset.loc[data_subset['Username'] == user].iloc[0]\n",
    "    user_data = user_data[['Username', 'country', 'track_count']].to_dict()\n",
    "    top_songs = data_subset.loc[data_subset['Username'] == user]\n",
    "    top_songs = top_songs[['rank', 'track_name', 'artist_name', 'playcount']]\n",
    "    users +=[User(user_data, top_songs)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from([\n",
    "    (p, {'name':p, \"node_type\" : \"user\"}) for p in users\n",
    "])\n",
    "G.add_nodes_from([\n",
    "    (t, {'name':t, \"node_type\" : \"track\"}) for t in tracks\n",
    "])\n",
    "\n",
    "# adding edges\n",
    "for user in users:\n",
    "  top_songs = user.top_songs\n",
    "  user_total_listening = user.total_playcount\n",
    "  for song, count in top_songs.values():\n",
    "    G.add_edge(user, song, weight=1 / (count/user_total_listening))\n",
    "\n",
    "\n",
    "# Make a large subgraph\n",
    "random.seed(225)\n",
    "rand_nodes_lg = random.sample(list(G.nodes()), 3000)\n",
    "sub_G_lg = G.subgraph(rand_nodes_lg)\n",
    "largest_cc_lg = max(nx.connected_components(sub_G_lg.to_undirected()), key=len)\n",
    "sub_G_lg = nx.Graph(sub_G_lg.subgraph(largest_cc_lg))\n",
    "print('Large subgraph Num nodes:', sub_G_lg.number_of_nodes(),\n",
    "      '. Num edges:', sub_G_lg.number_of_edges())\n",
    "\n",
    "\n",
    "n_nodes, n_edges = G.number_of_nodes(), G.number_of_edges()\n",
    "\n",
    "# by sorting them we get an ordering playlist1, ..., playlistN, track1, ..., trackN\n",
    "sorted_nodes = list(G.nodes())\n",
    "\n",
    "# create dictionaries to index to 0 to n_nodes, will be necessary for when we are using tensors\n",
    "node2id = dict(zip(sorted_nodes, np.arange(n_nodes)))\n",
    "id2node = dict(zip(np.arange(n_nodes), sorted_nodes))\n",
    "\n",
    "G = nx.relabel_nodes(G, node2id)\n",
    "\n",
    "# also keep track of how many users, tracks we have\n",
    "users_idx = [i for i, v in enumerate(node2id.keys()) if isinstance(v, User)] \n",
    "tracks_idx = [i for i, v in enumerate(node2id.keys()) if isinstance(v, Track)]\n",
    "n_users = np.max(users_idx) + 1\n",
    "n_tracks = n_nodes - n_users\n",
    "\n",
    "n_users, n_tracks\n",
    "\n",
    "# turn the graph into a torch_geometric Data object\n",
    "num_nodes = G.number_of_nodes()\n",
    "edge_idx = torch.Tensor(np.array(G.edges()).T)\n",
    "# Get the edge weights from the NetworkX graph\n",
    "edge_weights = []\n",
    "for u, v in G.edges():\n",
    "    edge_weights.append(G[u][v]['weight'])\n",
    "edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "print(edge_weights)\n",
    "graph_data = Data(edge_index = edge_idx, edge_weight = edge_weights, num_nodes = num_nodes)\n",
    "\n",
    "# convert to train/val/test splits\n",
    "transform = RandomLinkSplit(\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=False,\n",
    "    neg_sampling_ratio=0,\n",
    "    num_val=0.15, num_test=0.15\n",
    ")\n",
    "train_split, val_split, test_split = transform(graph_data)\n",
    "for split_data in [train_split, val_split, test_split]:\n",
    "    edge_weights_split = []\n",
    "    for u, v in split_data.edge_index.T:\n",
    "        edge_weights_split.append(G[node2id[id2node[u.item()]]][node2id[id2node[v.item()]]]['weight'])\n",
    "    split_data.edge_weight = torch.tensor(edge_weights_split, dtype=torch.float)\n",
    "\n",
    "\n",
    "# note these are stored as float32, we need them to be int64 for future training\n",
    "\n",
    "# Edge index: message passing edges\n",
    "train_split.edge_index = train_split.edge_index.type(torch.int64)\n",
    "val_split.edge_index = val_split.edge_index.type(torch.int64)\n",
    "test_split.edge_index = test_split.edge_index.type(torch.int64)\n",
    "# Edge label index: supervision edges\n",
    "train_split.edge_label_index = train_split.edge_label_index.type(torch.int64)\n",
    "val_split.edge_label_index = val_split.edge_label_index.type(torch.int64)\n",
    "test_split.edge_label_index = test_split.edge_label_index.type(torch.int64)\n",
    "\n",
    "print(f\"Train set has {train_split.edge_label_index.shape[1]} positives supervision edges\")\n",
    "print(f\"Validation set has {val_split.edge_label_index.shape[1]} positive supervision edges\")\n",
    "print(f\"Test set has {test_split.edge_label_index.shape[1]} positive supervision edges\")\n",
    "\n",
    "print(f\"Train set has {train_split.edge_index.shape[1]} message passing edges\")\n",
    "print(f\"Validation set has {val_split.edge_index.shape[1]} message passing edges\")\n",
    "print(f\"Test set has {test_split.edge_index.shape[1]} message passing edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "      Here we adapt the LightGCN model from Torch Geometric for our purposes. We allow\n",
    "      for customizable convolutional layers, custom embeddings. In addition, we deifne some\n",
    "      additional custom functions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int,\n",
    "        alpha: Optional[Union[float, Tensor]] = None,\n",
    "        alpha_learnable = False,\n",
    "        conv_layer = \"LGC\",\n",
    "        name = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        alpha_string = \"alpha\" if alpha_learnable else \"\"\n",
    "        self.name = f\"LGCN_{conv_layer}_{num_layers}_e{embedding_dim}_nodes{num_nodes}_{alpha_string}\"\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if alpha_learnable == True:\n",
    "          alpha_vals = torch.rand(num_layers+1)\n",
    "          alpha = nn.Parameter(alpha_vals/torch.sum(alpha_vals))\n",
    "          print(f\"Alpha learnable, initialized to: {alpha.softmax(dim=-1)}\")\n",
    "        else:\n",
    "          if alpha is None:\n",
    "              alpha = 1. / (num_layers + 1)\n",
    "\n",
    "          if isinstance(alpha, Tensor):\n",
    "              assert alpha.size(0) == num_layers + 1\n",
    "          else:\n",
    "              alpha = torch.tensor([alpha] * (num_layers + 1))\n",
    "\n",
    "        self.register_buffer('alpha', alpha)\n",
    "\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "\n",
    "        # initialize convolutional layers\n",
    "        self.conv_layer = conv_layer\n",
    "        if conv_layer == \"LGC\":\n",
    "          self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
    "        elif conv_layer == \"GAT\":\n",
    "          # initialize Graph Attention layer with multiple heads\n",
    "          # initialize linear layers to aggregate heads\n",
    "          n_heads = 5\n",
    "          self.convs = ModuleList(\n",
    "              [GATConv(in_channels = embedding_dim, out_channels = embedding_dim, heads = n_heads, dropout = 0.5, **kwargs) for _ in range(num_layers)]\n",
    "          )\n",
    "          self.linears = ModuleList([Linear(n_heads * embedding_dim, embedding_dim) for _ in range(num_layers)])\n",
    "\n",
    "        elif conv_layer == \"SAGE\":\n",
    "          #  initialize GraphSAGE conv\n",
    "          self.convs = ModuleList(\n",
    "              [SAGEConv(in_channels = embedding_dim, out_channels = embedding_dim, **kwargs) for _ in range(num_layers)]\n",
    "          )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def get_embedding(self, edge_index: Adj, edge_weight: OptTensor = None) -> Tensor:\n",
    "        x = self.embedding.weight\n",
    "\n",
    "        weights = self.alpha.softmax(dim=-1)\n",
    "        out = x * weights[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_weight)\n",
    "            if self.conv_layer == \"GAT\":\n",
    "              x = self.linears[i](x)\n",
    "            out = out + x * weights[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def initialize_embeddings(self, data):\n",
    "      # initialize with the data node features\n",
    "        self.embedding.weight.data.copy_(data.node_feature)\n",
    "\n",
    "\n",
    "    def forward(self, edge_index: Adj,\n",
    "                edge_label_index: OptTensor = None, edge_weight: OptTensor = None) -> Tensor:\n",
    "        if edge_label_index is None:\n",
    "            if isinstance(edge_index, SparseTensor):\n",
    "                edge_label_index = torch.stack(edge_index.coo()[:2], dim=0)\n",
    "            else:\n",
    "                edge_label_index = edge_index\n",
    "\n",
    "        out = self.get_embedding(edge_index, edge_weight)\n",
    "\n",
    "        return self.predict_link_embedding(out, edge_label_index)\n",
    "\n",
    "    def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
    "                     edge_weight: OptTensor = None,\n",
    "                     prob: bool = False) -> Tensor:\n",
    "\n",
    "        pred = self(edge_index, edge_label_index, edge_weight).sigmoid()\n",
    "        return pred if prob else pred.round()\n",
    "\n",
    "    def predict_link_embedding(self, embed: Adj, edge_label_index: Adj) -> Tensor:\n",
    "\n",
    "        embed_src = embed[edge_label_index[0]]\n",
    "        embed_dst = embed[edge_label_index[1]]\n",
    "        return (embed_src * embed_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "    def recommend(self, edge_index: Adj, edge_weight: OptTensor = None, src_index: OptTensor = None,\n",
    "                  dst_index: OptTensor = None, k: int = 1, sorted: bool = True) -> Tensor:\n",
    "        out_src = out_dst = self.get_embedding(edge_index, edge_weight)\n",
    "\n",
    "        if src_index is not None:\n",
    "            out_src = out_src[src_index]\n",
    "\n",
    "        if dst_index is not None:\n",
    "            out_dst = out_dst[dst_index]\n",
    "\n",
    "        pred = out_src @ out_dst.t()\n",
    "        top_index = pred.topk(k, dim=-1, sorted=sorted).indices\n",
    "\n",
    "        if dst_index is not None:  # Map local top-indices to original indices.\n",
    "            top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
    "\n",
    "        return top_index\n",
    "\n",
    "\n",
    "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
    "                       **kwargs) -> Tensor:\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "\n",
    "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
    "                            node_id: Optional[Tensor] = None, lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
    "        emb = self.embedding.weight\n",
    "        emb = emb if node_id is None else emb[node_id]\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, emb)\n",
    "\n",
    "    def bpr_loss(self, pos_scores, neg_scores):\n",
    "      return - torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
    "                f'{self.embedding_dim}, num_layers={self.num_layers})')\n",
    "\n",
    "\n",
    "\n",
    "class BPRLoss(_Loss):\n",
    "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
    "    observed entry to be higher than its unobserved counterparts\n",
    "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
    "\n",
    "    .. math::\n",
    "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
    "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
    "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
    "\n",
    "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
    "    We compute the mean BPR loss for simplicity.\n",
    "\n",
    "    Args:\n",
    "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
    "            (default: 0).\n",
    "        **kwargs (optional): Additional arguments of the underlying\n",
    "            :class:`torch.nn.modules.loss._Loss` class.\n",
    "    \"\"\"\n",
    "    __constants__ = ['lambda_reg']\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor,\n",
    "                parameters: Tensor = None) -> Tensor:\n",
    "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
    "            in the :obj:`negatives` entry should correspond to the same\n",
    "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        n_pairs = positives.size(0)\n",
    "        log_prob = F.logsigmoid(positives - negatives).sum()\n",
    "        regularization = 0\n",
    "\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "\n",
    "        return (-log_prob + regularization) / n_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negative_edges_nocheck(data, num_users, num_tracks, device = None):\n",
    "  # note computationally inefficient to check that these are indeed negative edges\n",
    "  users = data.edge_label_index[0, :]\n",
    "  tracks = torch.randint(num_users, num_users + num_tracks - 1, size = data.edge_label_index[1, :].size())\n",
    "\n",
    "  if users.get_device() != -1: # on gpu\n",
    "    tracks = tracks.to(device)\n",
    "\n",
    "  neg_edge_index = torch.stack((users, tracks), dim = 0)\n",
    "  neg_edge_label = torch.zeros(neg_edge_index.shape[1])\n",
    "\n",
    "  if neg_edge_index.get_device() != -1: # on gpu\n",
    "    neg_edge_label = neg_edge_label.to(device)\n",
    "\n",
    "  return neg_edge_index, neg_edge_label\n",
    "\n",
    "def sample_negative_edges(data, num_users, num_tracks, device=None):\n",
    "    positive_users, positive_tracks = data.edge_label_index\n",
    "\n",
    "    # Create a mask tensor with the shape (num_playlists, num_tracks)\n",
    "    mask = torch.zeros(num_users, num_tracks, device=device, dtype=torch.bool)\n",
    "    mask[positive_users, positive_tracks - num_users] = True\n",
    "\n",
    "    # Flatten the mask tensor and get the indices of the negative edges\n",
    "    flat_mask = mask.flatten()\n",
    "    negative_indices = torch.where(~flat_mask)[0]\n",
    "\n",
    "    # Sample negative edges from the negative_indices tensor\n",
    "    sampled_negative_indices = negative_indices[\n",
    "        torch.randint(0, negative_indices.size(0), size=(positive_users.size(0),), device=device)\n",
    "    ]\n",
    "\n",
    "    # Convert the indices back to playlists and tracks tensors\n",
    "    playlists = torch.floor_divide(sampled_negative_indices, num_tracks)\n",
    "    tracks = torch.remainder(sampled_negative_indices, num_tracks)\n",
    "    tracks = tracks + num_users\n",
    "\n",
    "    neg_edge_index = torch.stack((playlists, tracks), dim=0)\n",
    "    neg_edge_label = torch.zeros(neg_edge_index.shape[1], device=device)\n",
    "\n",
    "    return neg_edge_index, neg_edge_label\n",
    "\n",
    "def sample_hard_negative_edges(data, model, num_users, num_tracks, device=None, batch_size=500, frac_sample = 1):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embedding(data.edge_index, data.edge_weight)\n",
    "        user_embeddings = embeddings[:num_users].to(device)\n",
    "        tracks_embeddings = embeddings[num_users:].to(device)\n",
    "\n",
    "    positive_users, positive_tracks = data.edge_label_index\n",
    "    num_edges = positive_users.size(0)\n",
    "\n",
    "    # Create a boolean mask for all the positive edges\n",
    "    positive_mask = torch.zeros(num_users, num_tracks, device=device, dtype=torch.bool)\n",
    "    positive_mask[positive_users, positive_tracks - num_users] = True\n",
    "\n",
    "    neg_edges_list = []\n",
    "    neg_edge_label_list = []\n",
    "\n",
    "    for batch_start in range(0, num_edges, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_edges)\n",
    "\n",
    "        batch_scores = torch.matmul(\n",
    "            user_embeddings[positive_users[batch_start:batch_end]], tracks_embeddings.t()\n",
    "        )\n",
    "\n",
    "        # Set the scores of the positive edges to negative infinity\n",
    "        batch_scores[positive_mask[positive_users[batch_start:batch_end]]] = -float(\"inf\")\n",
    "\n",
    "        # Select the top k highest scoring negative edges for each playlist in the current batch\n",
    "        # do 0.99 to filter out all pos edges which will be at the end\n",
    "        _, top_indices = torch.topk(batch_scores, int(frac_sample * 0.99 * num_tracks), dim=1)\n",
    "        selected_indices = torch.randint(0, int(frac_sample * 0.99 *num_tracks), size = (batch_end - batch_start, ))\n",
    "        top_indices_selected = top_indices[torch.arange(batch_end - batch_start), selected_indices] + n_users\n",
    "\n",
    "        # Create the negative edges tensor for the current batch\n",
    "        neg_edges_batch = torch.stack(\n",
    "            (positive_users[batch_start:batch_end], top_indices_selected), dim=0\n",
    "        )\n",
    "        neg_edge_label_batch = torch.zeros(neg_edges_batch.shape[1], device=device)\n",
    "\n",
    "        neg_edges_list.append(neg_edges_batch)\n",
    "        neg_edge_label_list.append(neg_edge_label_batch)\n",
    "\n",
    "    # Concatenate the batch tensors\n",
    "    neg_edges = torch.cat(neg_edges_list, dim=1)\n",
    "    neg_edge_label = torch.cat(neg_edge_label_list)\n",
    "\n",
    "    return neg_edges, neg_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(data, model, k = 30, batch_size = 64, device = None):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embedding(data.edge_index, data.edge_weight)\n",
    "        user_embeddings = embeddings[:n_users]\n",
    "        tracks_embeddings = embeddings[n_users:]\n",
    "\n",
    "    hits_list = []\n",
    "    relevant_counts_list = []\n",
    "\n",
    "    for batch_start in range(0, n_users, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, n_users)\n",
    "        batch_users_embeddings = user_embeddings[batch_start:batch_end]\n",
    "\n",
    "        # Calculate scores for all possible item pairs\n",
    "        scores = torch.matmul(batch_users_embeddings, tracks_embeddings.t())\n",
    "\n",
    "        # Set the scores of message passing edges to negative infinity\n",
    "        mp_indices = ((data.edge_index[0] >= batch_start) & (data.edge_index[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
    "        scores[data.edge_index[0, mp_indices] - batch_start, data.edge_index[1, mp_indices] - n_users] = -float(\"inf\")\n",
    "\n",
    "        # Find the top k highest scoring items for each playlist in the batch\n",
    "        _, top_k_indices = torch.topk(scores, k, dim=1)\n",
    "\n",
    "        # Ground truth supervision edges\n",
    "        ground_truth_edges = data.edge_label_index\n",
    "\n",
    "        # Create a mask to indicate if the top k items are in the ground truth supervision edges\n",
    "        mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
    "        gt_indices = ((ground_truth_edges[0] >= batch_start) & (ground_truth_edges[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
    "        mask[ground_truth_edges[0, gt_indices] - batch_start, ground_truth_edges[1, gt_indices] - n_users] = True\n",
    "\n",
    "        # Check how many of the top k items are in the ground truth supervision edges\n",
    "        hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
    "        hits_list.append(hits)\n",
    "\n",
    "        # Calculate the total number of relevant items for each playlist in the batch\n",
    "        relevant_counts = torch.bincount(ground_truth_edges[0, gt_indices] - batch_start, minlength=batch_end - batch_start)\n",
    "        relevant_counts_list.append(relevant_counts)\n",
    "\n",
    "    # Compute recall@k\n",
    "    hits_tensor = torch.cat(hits_list, dim=0)\n",
    "    relevant_counts_tensor = torch.cat(relevant_counts_list, dim=0)\n",
    "    # Handle division by zero case\n",
    "    recall_at_k = torch.where(\n",
    "        relevant_counts_tensor != 0,\n",
    "        hits_tensor.true_divide(relevant_counts_tensor),\n",
    "        torch.ones_like(hits_tensor)\n",
    "    )\n",
    "    # take average\n",
    "    recall_at_k = torch.mean(recall_at_k)\n",
    "\n",
    "    if recall_at_k.numel() == 1:\n",
    "        return recall_at_k.item()\n",
    "    else:\n",
    "        raise ValueError(\"recall_at_k contains more than one item.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(labels, preds):\n",
    "  roc = roc_auc_score(labels.flatten().cpu().numpy(), preds.flatten().data.cpu().numpy())\n",
    "  return roc\n",
    "# Train\n",
    "def train(datasets, model, optimizer, loss_fn, args, K = 30, neg_samp = \"random\"):\n",
    "  # print(f\"Beginning training for {model.name}\")\n",
    "\n",
    "  train_data = datasets[\"train\"]\n",
    "  val_data = datasets[\"val\"]\n",
    "\n",
    "  stats = {\n",
    "      'train': {\n",
    "        'loss': [],\n",
    "        'roc' : []\n",
    "      },\n",
    "      'val': {\n",
    "        'loss': [],\n",
    "        'recall': [],\n",
    "        'roc' : []\n",
    "      }\n",
    "\n",
    "  }\n",
    "  val_neg_edge, val_neg_label = None, None\n",
    "  for epoch in range(args[\"epochs\"]): # loop over each epoch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # obtain negative sample\n",
    "    if neg_samp == \"random\":\n",
    "      neg_edge_index, neg_edge_label = sample_negative_edges(train_data, n_users, n_tracks, args[\"device\"])\n",
    "    elif neg_samp == \"hard\":\n",
    "      if epoch % 5 == 0:\n",
    "        neg_edge_index, neg_edge_label = sample_hard_negative_edges(\n",
    "            train_data, model, n_users, n_tracks, args[\"device\"], batch_size = 500,\n",
    "            frac_sample = 1 - (0.5 * epoch / args[\"epochs\"])\n",
    "        )\n",
    "    # calculate embedding\n",
    "    embed = model.get_embedding(train_data.edge_index, train_data.edge_weight)\n",
    "    # calculate pos, negative scores using embedding\n",
    "    pos_scores = model.predict_link_embedding(embed, train_data.edge_label_index)\n",
    "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
    "\n",
    "    # concatenate pos, neg scores together and evaluate loss\n",
    "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
    "    labels = torch.cat((train_data.edge_label, neg_edge_label), dim = 0)\n",
    "\n",
    "    # calculate loss function\n",
    "    if loss_fn == \"BCE\":\n",
    "      # edge_weight = train_data.edge_weight[train_data.edge_label_index].view(-1)\n",
    "      loss = weighted_binary_cross_entropy(scores[:len(pos_scores)], labels[:len(pos_scores)], train_data.edge_weight)\n",
    "      loss = model.link_pred_loss(scores, labels)\n",
    "    elif loss_fn == \"BPR\":\n",
    "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
    "\n",
    "    train_roc = metrics(labels, scores)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    val_loss, val_roc, val_neg_edge, val_neg_label = test(\n",
    "        model, val_data, loss_fn, neg_samp, epoch, val_neg_edge, val_neg_label\n",
    "    )\n",
    "\n",
    "    stats['train']['loss'].append(loss)\n",
    "    stats['train']['roc'].append(train_roc)\n",
    "    stats['val']['loss'].append(val_loss)\n",
    "    stats['val']['roc'].append(val_roc)\n",
    "\n",
    "    print(f\"Epoch {epoch}; Train loss {loss}; Val loss {val_loss}; Train ROC {train_roc}; Val ROC {val_roc}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      # calculate recall @ K\n",
    "      val_recall = recall_at_k(val_data, model, k = K, device = args[\"device\"])\n",
    "      print(f\"Val recall {val_recall}\")\n",
    "      stats['val']['recall'].append(val_recall)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "\n",
    "      # save embeddings for future visualization\n",
    "      path = os.path.join(\"model_embeddings\", model.name)\n",
    "      if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "      torch.save(model.embedding.weight, os.path.join(\"model_embeddings\", model.name, f\"{model.name}_{loss_fn}_{neg_samp}_{epoch}.pt\"))\n",
    "\n",
    "  pickle.dump(stats, open(f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pkl\", \"wb\"))\n",
    "  return stats\n",
    "\n",
    "def test(model, data, loss_fn, neg_samp, epoch = 0, neg_edge_index = None, neg_edge_label = None):\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad(): # want to save RAM\n",
    "\n",
    "    # conduct negative sampling\n",
    "    if neg_samp == \"random\":\n",
    "      neg_edge_index, neg_edge_label = sample_negative_edges(data, n_users, n_tracks, args[\"device\"])\n",
    "    elif neg_samp == \"hard\":\n",
    "      if epoch % 5 == 0 or neg_edge_index is None:\n",
    "        neg_edge_index, neg_edge_label = sample_hard_negative_edges(\n",
    "            data, model, n_users, n_tracks, args[\"device\"], batch_size = 500,\n",
    "            frac_sample = 1 - (0.5 * epoch / args[\"epochs\"])\n",
    "        )\n",
    "    # obtain model embedding\n",
    "    embed = model.get_embedding(data.edge_index, data.edge_weight)\n",
    "    # calculate pos, neg scores using embedding\n",
    "    pos_scores = model.predict_link_embedding(embed, data.edge_label_index)\n",
    "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
    "    # concatenate pos, neg scores together and evaluate loss\n",
    "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
    "    labels = torch.cat((data.edge_label, neg_edge_label), dim = 0)\n",
    "    # calculate loss\n",
    "    if loss_fn == \"BCE\":\n",
    "      loss = model.link_pred_loss(scores, labels)\n",
    "    elif loss_fn == \"BPR\":\n",
    "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
    "\n",
    "    roc = metrics(labels, scores)\n",
    "\n",
    "  return loss, roc, neg_edge_index, neg_edge_label\n",
    "\n",
    "def weighted_binary_cross_entropy(output, target, edge_weight):\n",
    "    # Apply sigmoid activation to ensure output is in [0, 1]\n",
    "    output = torch.sigmoid(output)\n",
    "\n",
    "    # Only apply edge weights to the positive scores\n",
    "    pos_output = output[:len(edge_weight)]\n",
    "    neg_output = output[len(edge_weight):]\n",
    "    \n",
    "    # Adjust the scaling factor and exponent as needed\n",
    "    scaling_factor = 10\n",
    "    exponent = -0.5\n",
    "    weight = scaling_factor * torch.pow(edge_weight[:len(pos_output)] + 1e-8, exponent)\n",
    "    \n",
    "    # Compute loss separately for positive and negative scores\n",
    "    pos_loss = F.binary_cross_entropy(pos_output, target[:len(edge_weight)], weight=weight, reduction='none')\n",
    "    neg_loss = F.binary_cross_entropy(neg_output, target[len(edge_weight):], reduction='none')\n",
    "    \n",
    "    # Combine the losses\n",
    "    loss = torch.cat([pos_loss, neg_loss], dim=0).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the dataset splits\n",
    "datasets = {\n",
    "    'train':train_split,\n",
    "    'val':val_split,\n",
    "    'test': test_split\n",
    "}\n",
    "\n",
    "# initialize our arguments\n",
    "args = {\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_layers' : 3,\n",
    "    'emb_size' : 64,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 301\n",
    "}\n",
    "\n",
    "# initialize model and and optimizer\n",
    "num_nodes = n_users + n_tracks\n",
    "model = GCN(\n",
    "    num_nodes = num_nodes, num_layers = args['num_layers'],\n",
    "    embedding_dim = args[\"emb_size\"]\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "# send data, model to GPU if available\n",
    "users_idx = torch.Tensor(users_idx).type(torch.int64).to(args[\"device\"])\n",
    "tracks_idx = torch.Tensor(tracks_idx).type(torch.int64).to(args[\"device\"])\n",
    "datasets['train'].to(args['device'])\n",
    "datasets['val'].to(args['device'])\n",
    "datasets['test'].to(args['device'])\n",
    "model.to(args[\"device\"])\n",
    "\n",
    "# create directory to save model_stats\n",
    "MODEL_STATS_DIR = \"model_stats\"\n",
    "if not os.path.exists(MODEL_STATS_DIR):\n",
    "  os.makedirs(MODEL_STATS_DIR)\n",
    "\n",
    "\n",
    "# can set BCE -> BPR also, see which is better\n",
    "train(datasets, model, optimizer, \"BCE\", args, neg_samp = \"random\")\n",
    "\n",
    "test(model, datasets['test'], \"BCE\", neg_samp = \"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(conv_layer, args, alpha = False):\n",
    "  num_nodes = n_users + n_tracks\n",
    "  model = GCN(\n",
    "      num_nodes = num_nodes, num_layers = args['num_layers'],\n",
    "      embedding_dim = args[\"emb_size\"], conv_layer = conv_layer,\n",
    "      alpha_learnable = alpha\n",
    "  )\n",
    "  model.to(args[\"device\"])\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "  return model, optimizer\n",
    "\n",
    "## For example:\n",
    "\n",
    "# using BPR loss\n",
    "loss_fn = \"BPR\"\n",
    "\n",
    "# using hard sampling\n",
    "neg_samp = \"hard\"\n",
    "\n",
    "# for LGConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"LGC\", args)\n",
    "lgc_stats_hard = train(datasets, model, optimizer, loss_fn, args, K = 30, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")\n",
    "\n",
    "# for GATConv:\n",
    "model, optimizer = init_model(\"GAT\", args)\n",
    "gat_stats_hard = train(datasets, model, optimizer, loss_fn, args,  K = 30, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")\n",
    "\n",
    "# for SAGEConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"SAGE\", args)\n",
    "sage_stats_hard = train(datasets, model, optimizer, loss_fn, args,  K = 30, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")\n",
    "\n",
    "# # using random sampling\n",
    "neg_samp = \"random\"\n",
    "\n",
    "# for LGConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"LGC\", args)\n",
    "lgc_stats = train(datasets, model, optimizer, loss_fn, args,  K = 30, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")\n",
    "\n",
    "# for GATConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"GAT\", args)\n",
    "gat_stats = train(datasets, model, optimizer, loss_fn, args,  K = 30, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")\n",
    "\n",
    "# for SAGEConv:\n",
    "args['epochs'] = 301\n",
    "args['num_layers'] = 3\n",
    "model, optimizer = init_model(\"SAGE\", args)\n",
    "sage_stats = train(datasets, model, optimizer, loss_fn, args,  K = 30, neg_samp = neg_samp)\n",
    "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
