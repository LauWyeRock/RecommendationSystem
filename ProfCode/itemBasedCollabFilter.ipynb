{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xSJsEA9ZNduh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "- This notebook file may contain methods or algorithms that are NOT covered by the teaching content of BT4222 and hence will not be assessed in your midterm exam.\n",
        "- It serves to increase your exposure in depth and breath to the practical methods in addressing the specific project topic. We believe it will be helpful for your current project and also your future internship endeavors."
      ],
      "metadata": {
        "id": "Ff-QjwP9nHJu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ovcCwswvdh"
      },
      "source": [
        "## Item Based Collaborative Filtering (IBCF)\n",
        "\n",
        "High level overview of IBCF is that we breakdown our data into two matrices\n",
        "\n",
        "- Item to item similarity matrix $S$\n",
        "- User to item affinity matrix $A$\n",
        "\n",
        "Recommendation scores are then created by computing the matrix multiplication $A\\times S$.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1RDVbfl8quKbGPJ3wOOwgfOFvSxFOlwIG\"></img>\n",
        "\n",
        "### Item to Item Similarity Matrix $S$\n",
        "\n",
        "We create a $m \\times m$ matrix where m is the number of item. Each element $c_{ij}$ in the matrix is the count of item $i$ and item $j$ occuring together.\n",
        "\n",
        "Similarity matrix has the following properties:\n",
        "\n",
        "- Symmetric, $c_{i,j} = c_{j,i}$\n",
        "- Non negative: $c_{i,j} \\geq 0$\n",
        "- The occurrences are at least as large as the co-occurrences. I.e., the largest element for each row (and column) is on the main diagonal: $\\forall(i,j) C_{i,i},C_{j,j} \\geq C_{i,j}$.\n",
        "\n",
        "We can then scale this matrix to get our final Similarity Matrix $S$ using the following techniques:\n",
        "\n",
        "- `Jaccard`: $s_{ij}=\\frac{c_{ij}}{(c_{ii}+c_{jj}-c_{ij})}$\n",
        "- `lift`: $s_{ij}=\\frac{c_{ij}}{(c_{ii} \\times c_{jj})}$\n",
        "- `counts`: $s_{ij}=c_{ij}$\n",
        "\n",
        "Intuition on which to select?\n",
        "- No scaling means that the most popular item will be recommended to the user\n",
        "- Lift will recommend items that are less popular but are highly favoured by a subset of users.\n",
        "- Jaccard finds a balance between the above two methods.\n",
        "\n",
        "### User to item affinity matrix $A$\n",
        "\n",
        "\n",
        "The affinity matrix in captures the strength of the relationship between each individual user and the items that user has already interacted with.\n",
        "\n",
        "We create a $n \\times m$ matrix where $n$ is the number of users and $m$ is the number of items. In thise matrix each element $a_{ij}$ tells you how much does user $i$ favour item $j$.\n",
        "\n",
        "$$a_{ij}=\\sum_k w_k \\left(\\frac{1}{2}\\right)^{\\frac{t_0-t_k}{T}} $$\n",
        "\n",
        "where the affinity $a_{ij}$ for user $i$ and item $j$ is the weighted sum of all $k$ events involving user $i$ and item $j$. $w_k$ represents the weight of a particular event, and the power of 2 term reflects the temporally-discounted event. The $(\\frac{1}{2})^n$ scaling factor causes the parameter $T$ to serve as a half-life: events $T$ units before $t_0$ will be given half the weight as those taking place at $t_0$.\n",
        "\n",
        "Repeating this computation for all $n$ users and $m$ items results in an $n\\times m$ matrix $A$. Simplifications of the above expression can be obtained by setting all the weights equal to 1 (effectively ignoring event types), or by setting the half-life parameter $T$ to infinity (ignoring transaction times).\n",
        "\n",
        "### Remove seen item\n",
        "\n",
        "Optionally we remove items which have already been seen in the training set, i.e. don't recommend items which have been previously bought by the user again.\n",
        "\n",
        "### Drawback\n",
        "\n",
        "Algorithm itself has $O(n^3)$ complexity. Therefore it is not supposed to handle large dataset in a scalable manner. Whenever one uses the algorithm, it is recommended to run with sufficiently large memory.\n",
        "\n",
        "<font size=1> Content of the notebook is taken from the following repository: https://github.com/microsoft/recommenders/tree/main/recommenders </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment (~4mins)\n"
      ],
      "metadata": {
        "id": "xSJsEA9ZNduh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DczE5pqyNSe9",
        "outputId": "4688d61b-8fb1-493a-db63-e7e423a56cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 07:42:59--  https://repo.anaconda.com/miniconda/Miniconda3-py39_23.5.2-0-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93409434 (89M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py39_23.5.2-0-Linux-x86_64.sh.2’\n",
            "\n",
            "Miniconda3-py39_23. 100%[===================>]  89.08M   219MB/s    in 0.4s    \n",
            "\n",
            "2023-10-27 07:43:00 (219 MB/s) - ‘Miniconda3-py39_23.5.2-0-Linux-x86_64.sh.2’ saved [93409434/93409434]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "                                                                                    \n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "Preparing transaction: - \b\bdone\n",
            "Executing transaction: | \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Requirement already satisfied: pyOpenSSL==22.0.0 in /usr/local/lib/python3.9/site-packages (22.0.0)\n",
            "Requirement already satisfied: cryptography>=35.0 in /usr/local/lib/python3.9/site-packages (from pyOpenSSL==22.0.0) (39.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/site-packages (from cryptography>=35.0->pyOpenSSL==22.0.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=35.0->pyOpenSSL==22.0.0) (2.21)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n",
            "    from pip._internal.utils.misc import ensure_dir\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_internal/utils/misc.py\", line 40, in <module>\n",
            "    from pip._vendor.tenacity import retry, stop_after_delay, wait_fixed\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_vendor/tenacity/__init__.py\", line 59, in <module>\n",
            "    from .wait import wait_chain  # noqa\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_vendor/tenacity/wait.py\", line 44, in <module>\n",
            "    WaitBaseT = typing.Union[wait_base, typing.Callable[[\"RetryCallState\"], typing.Union[float, int]]]\n",
            "  File \"/usr/local/lib/python3.9/typing.py\", line 274, in inner\n",
            "    return cached(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/typing.py\", line 354, in __getitem__\n",
            "    return self._getitem(self, parameters)\n",
            "  File \"/usr/local/lib/python3.9/typing.py\", line 470, in Union\n",
            "    return _UnionGenericAlias(self, parameters)\n",
            "  File \"/usr/local/lib/python3.9/typing.py\", line 743, in __init__\n",
            "    self.__args__ = tuple(... if a is _TypingEllipsis else\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip\", line 7, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.9/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!sudo rm -rf /usr/local/lib/python3.8/dist-packages/OpenSSL\n",
        "!sudo rm -rf /usr/local/lib/python3.8/dist-packages/pyOpenSSL-22.1.0.dist-info/\n",
        "\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-py39_23.5.2-0-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py39_23.5.2-0-Linux-x86_64.sh\n",
        "\n",
        "!bash ./Miniconda3-py39_23.5.2-0-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.9/site-packages/')\n",
        "!pip3 install pyOpenSSL==22.0.0\n",
        "\n",
        "!pip install recommenders[examples]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "YjXVT2U-Pf1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the environment path to find Recommenders\n",
        "import sys\n",
        "import logging\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from recommenders.datasets import movielens\n",
        "from recommenders.datasets.python_splitters import python_stratified_split\n",
        "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k, serendipity, diversity, catalog_coverage, distributional_coverage, novelty\n",
        "from recommenders.models.sar import SAR\n",
        "\n",
        "print(f\"System version: {sys.version}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"SciPy version: {scipy.__version__}\")"
      ],
      "metadata": {
        "id": "ashCGwePepde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset"
      ],
      "metadata": {
        "id": "2YGZeqJ4p7Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top k items to recommend\n",
        "TOP_K = 10\n",
        "\n",
        "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
        "MOVIELENS_DATA_SIZE = \"100k\""
      ],
      "metadata": {
        "id": "Z2fVIU_4evMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = movielens.load_pandas_df(\n",
        "    size=MOVIELENS_DATA_SIZE,\n",
        "    header=[\"UserId\", \"MovieId\", \"Rating\", \"Timestamp\"],\n",
        "    title_col=\"Title\",\n",
        ")\n",
        "\n",
        "# Convert the float precision to 32-bit in order to reduce memory consumption\n",
        "data[\"Rating\"] = data[\"Rating\"].astype(np.float32)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "8qiyZ2f-e16C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = {\n",
        "    \"col_user\": \"UserId\",\n",
        "    \"col_item\": \"MovieId\",\n",
        "    \"col_rating\": \"Rating\",\n",
        "    \"col_timestamp\": \"Timestamp\",\n",
        "    \"col_prediction\": \"Prediction\",\n",
        "}\n",
        "\n",
        "#Split the dataset into 75% train and 25% test\n",
        "\n",
        "train, test = python_stratified_split(\n",
        "    data, ratio=0.75, col_user=header[\"col_user\"], col_item=header[\"col_item\"], seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "-KWGhcjQe5bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "nFZ7okrjqCV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the model using the Jaccard Similarity method. This will find a balance between recommending popular items and obscure items.\n",
        "\n",
        "model = SAR(\n",
        "    similarity_type=\"jaccard\",\n",
        "    time_decay_coefficient=30,\n",
        "    time_now=None,\n",
        "    timedecay_formula=True,\n",
        "    **header\n",
        ")"
      ],
      "metadata": {
        "id": "ZJzIRWTlfdG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model on the training data and computing the matrices.\n",
        "model.fit(train)"
      ],
      "metadata": {
        "id": "P_81tMGNfgR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "Ks9SZGqfqPBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting top k items for every user.\n",
        "# We are not recommending items that have been rated by the user.\n",
        "top_k = model.recommend_k_items(test, top_k=TOP_K, remove_seen=True)"
      ],
      "metadata": {
        "id": "v47310rNfjbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link movie id's to movie names.\n",
        "\n",
        "top_k_with_titles = top_k.join(\n",
        "    data[[\"MovieId\", \"Title\"]].drop_duplicates().set_index(\"MovieId\"),\n",
        "    on=\"MovieId\",\n",
        "    how=\"inner\",\n",
        ").sort_values(by=[\"UserId\", \"Prediction\"], ascending=False)\n",
        "\n",
        "top_k_with_titles.head(10)"
      ],
      "metadata": {
        "id": "rQnP3itWfqTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining arguments for calculating metrics. All ranking based metrics have the same arguments\n",
        "args = [test, top_k]\n",
        "kwargs = dict(\n",
        "    col_user=\"UserId\",\n",
        "    col_item=\"MovieId\",\n",
        "    col_rating=\"Rating\",\n",
        "    col_prediction=\"Prediction\",\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=TOP_K,\n",
        ")"
      ],
      "metadata": {
        "id": "esfB6hNkqtXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ranking Metrics"
      ],
      "metadata": {
        "id": "7IkS8V4i523q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### MAP\n",
        "\n",
        "It is the average precision for each user normalized over all users."
      ],
      "metadata": {
        "id": "gYTotvpnYsLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_map = map_at_k(*args, **kwargs)\n",
        "print(f\"MAP: {eval_map}\")"
      ],
      "metadata": {
        "id": "opVrJV2fsZia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### NDCG\n",
        "\n",
        "Normalized Discounted Cumulative Gain (NDCG) - evaluates how well the predicted items for a user are ranked based on relevance\n",
        "\n",
        "<font size=\"1\"> For more information visit https://medium.com/@readsumant/understanding-ndcg-as-a-metric-for-your-recomendation-system-5cd012fb3397 <font>"
      ],
      "metadata": {
        "id": "WE43tMC4YwXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_ndcg = ndcg_at_k(*args, **kwargs)\n",
        "print(f\"NDCG: {eval_ndcg}\")"
      ],
      "metadata": {
        "id": "LWJZI93jn6jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Precision Recall\n",
        "\n",
        "Precision - this measures the proportion of recommended items that are relevant\n",
        "\n",
        "Recall - this measures the proportion of relevant items that are recommended"
      ],
      "metadata": {
        "id": "lMECdyAPYyV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_precision = precision_at_k(*args, **kwargs)\n",
        "eval_recall = recall_at_k(*args, **kwargs)\n",
        "print(f\"Precision: {eval_precision} \\nRecall: {eval_recall}\")"
      ],
      "metadata": {
        "id": "_FWpwaikn-6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Diversity Metrics"
      ],
      "metadata": {
        "id": "jbJMWWuz6Fwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Coverage"
      ],
      "metadata": {
        "id": "axO6bG5k6JDa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAp8uiFlwNwY"
      },
      "source": [
        "\n",
        "We define _catalog coverage_ as the proportion of items showing in all users’ recommendations:\n",
        "$$\n",
        "\\textrm{CatalogCoverage} = \\frac{|N_r|}{|N_t|}\n",
        "$$\n",
        "where $N_r$ denotes the set of items in the recommendations (`top_k` in the code below) and $N_t$ the set of items in the historical data (`train`).\n",
        "\n",
        "_Distributional coverage_ measures how equally different items are recommended to users when a particular recommender system is used.\n",
        "If  $p(i|R)$ denotes the probability that item $i$ is observed among all recommendation lists, we define distributional coverage as\n",
        "$$\n",
        "\\textrm{DistributionalCoverage} = -\\sum_{i \\in N_t} p(i|R) \\log_2 p(i)\n",
        "$$\n",
        "where\n",
        "$$\n",
        "p(i|R) = \\frac{|M_r (i)|}{|\\textrm{train}|}\n",
        "$$\n",
        "and $M_r (i)$ denotes the users who are recommended item $i$.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cov_args = [train, top_k]\n",
        "cov_kwargs = dict(\n",
        "    col_user=\"UserId\",\n",
        "    col_item=\"MovieId\",\n",
        ")\n",
        "cat_coverage = catalog_coverage(*cov_args, **cov_kwargs)\n",
        "dist_coverage = distributional_coverage(*cov_args, **cov_kwargs)\n",
        "print(f\"Catalog Coverage: {cat_coverage} \\nDistributional Coverage: {dist_coverage}\")"
      ],
      "metadata": {
        "id": "6FVJ2jLKsACj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Diversity"
      ],
      "metadata": {
        "id": "i9xRbFkT6NKo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBHcHFGBwNwZ"
      },
      "source": [
        "\n",
        "**Diversity**\n",
        "\n",
        "Diversity represents the variety present in a list of recommendations.\n",
        "_Intra-List Similarity_ aggregates the pairwise similarity of all items in a set. A recommendation list with groups of very similar items will score a high intra-list similarity. Lower intra-list similarity indicates higher diversity.\n",
        "To measure similarity between any two items we use _cosine similarity_:\n",
        "$$\n",
        "\\textrm{Cosine Similarity}(i,j)=  \\frac{|M_t^{l(i,j)}|} {\\sqrt{|M_t^{l(i)}|} \\sqrt{|M_t^{l(j)}|} }\n",
        "$$\n",
        "where $M_t^{l(i)}$ denotes the set of users who liked item $i$ and $M_t^{l(i,j)}$ the users who liked both $i$ and $j$.\n",
        "Intra-list similarity is then defined as\n",
        "$$\n",
        "\\textrm{IL} = \\frac{1}{|M|} \\sum_{u \\in M} \\frac{1}{\\binom{N_r(u)}{2}} \\sum_{i,j \\in N_r (u),\\, i<j} \\textrm{Cosine Similarity}(i,j)\n",
        "$$\n",
        "where $M$ is the set of users and $N_r(u)$ the set of recommendations for user $u$. Finally, diversity is defined as\n",
        "$$\n",
        "\\textrm{diversity} = 1 - \\textrm{IL}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "div_args = [train, top_k]\n",
        "div_kwargs = dict(\n",
        "    col_user=\"UserId\",\n",
        "    col_item=\"MovieId\",\n",
        ")\n",
        "diversity_eval = diversity(*div_args, **div_kwargs)\n",
        "print(f\"Diversity: {diversity_eval}\")"
      ],
      "metadata": {
        "id": "zAAbWpFb6ksW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Novelty"
      ],
      "metadata": {
        "id": "pToxuPiB6Psb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAwL5fdnwNwZ"
      },
      "source": [
        "\n",
        "**Novelty**\n",
        "\n",
        "The novelty of an item is inverse to its _popularity_. If $p(i)$ represents the probability that item $i$ is observed (or known, interacted with etc.) by users, then  \n",
        "$$\n",
        "p(i) = \\frac{|M_t (i)|} {|\\textrm{train}|}\n",
        "$$\n",
        "where $M_t (i)$ is the set of users who have interacted with item $i$ in the historical data.\n",
        "\n",
        "The novelty of an item is then defined as\n",
        "$$\n",
        "\\textrm{novelty}(i) = -\\log_2 p(i)\n",
        "$$\n",
        "and the novelty of the recommendations across all users is defined as\n",
        "$$\n",
        "\\textrm{novelty} = \\sum_{i \\in N_r} \\frac{|M_r (i)|}{|\\textrm{top_k}|} \\textrm{novelty}(i)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nov_args = [train, top_k]\n",
        "nov_kwargs = dict(\n",
        "    col_user=\"UserId\",\n",
        "    col_item=\"MovieId\",\n",
        ")\n",
        "novelty_eval = novelty(*nov_args, **nov_kwargs)\n",
        "print(f\"Novelty: {novelty_eval}\")"
      ],
      "metadata": {
        "id": "NRkTRmvr60AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Serendipity"
      ],
      "metadata": {
        "id": "eI__vl4U6RMz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT4jbTe6wNwa"
      },
      "source": [
        "**Serendipity**\n",
        "\n",
        "Serendipity represents the “unusualness” or “surprise” of recommendations. Unlike novelty, serendipity encompasses the semantic content of items and can be imagined as the distance between recommended items and their expected contents (Zhang et al.) Lower cosine similarity indicates lower expectedness and higher serendipity.\n",
        "We define the expectedness of an unseen item $i$ for user $u$ as the average similarity between every already seen item $j$ in the historical data and $i$:\n",
        "$$\n",
        "\\textrm{expectedness}(i|u) = \\frac{1}{|N_t (u)|} \\sum_{j \\in N_t (u)} \\textrm{Cosine Similarity}(i,j)\n",
        "$$\n",
        "The serendipity of item $i$ is (1 - expectedness) multiplied by _relevance_, where relevance indicates whether the item turns out to be liked by the user or not. For example, in a binary scenario, if an item in `top_k` is liked (purchased, clicked) in `train`, its relevance equals one, otherwise it equals zero. Aggregating over all users and items, the overall\n",
        "serendipity is defined as\n",
        "$$\n",
        "\\textrm{serendipity} = \\frac{1}{|M|} \\sum_{u \\in M_r}\n",
        "\\frac{1}{|N_r (u)|} \\sum_{i \\in N_r (u)} \\big(1 - \\textrm{expectedness}(i|u) \\big) \\, \\textrm{relevance}(i)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ser_args = [train, top_k]\n",
        "ser_kwargs = dict(\n",
        "    col_user=\"UserId\",\n",
        "    col_item=\"MovieId\",\n",
        ")\n",
        "ser_eval = serendipity(*ser_args, **ser_kwargs)\n",
        "print(f\"Serendipity: {ser_eval}\")"
      ],
      "metadata": {
        "id": "cX3lKa8V7Bvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Ranking Metrics"
      ],
      "metadata": {
        "id": "juJBKSw27zza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "|Metric|Range|Selection criteria|Limitation|\n",
        "|------|-------------------------------|---------|----------|\n",
        "|Precision|$\\geq 0$ and $\\leq 1$|Higher the better.|Only for hits in recommendations.|\n",
        "|Recall|$\\geq 0$ and $\\leq 1$|Higher the better.|Only for hits in the ground truth.|\n",
        "|NDCG|$\\geq 0$ and $\\leq 1$|Higher the better.|Does not penalize for bad/missing items, and does not perform for several equally good items.|\n",
        "|MAP|$\\geq 0$ and $\\leq 1$|Higher the better.|Depend on variable distributions.|\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "K8wZUZll2VC-"
      }
    }
  ]
}