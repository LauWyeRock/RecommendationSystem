{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import base64   \n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import lyricsgenius\n",
    "import langdetect\n",
    "import re\n",
    "import string\n",
    "import tempfile\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any api we can try using the \"+\" email trick to get more API keys\n",
    "\n",
    "genius_client_id = \"wZZ2RWc5mqp-5Pbz2W1rQJWE8LQ3pFBrb1Hw5_AOqgybq28mt7kjdjcG4zktCNbO\"\n",
    "genius_client_secret = \"PefqBJHor_muDgTutGlaXXaxmzsI7TQCps9FQ3FwkUTT0WJIT3s0A5YA9mnFbfp_-CBhQF7b0omgE8kaM3dJ3w\"\n",
    "genius_access_token = \"NUHHVpwnmbDYUYw8Padu0gQeHvYN4OsKYtE2MKNUpBUI6yR-xZXKY6S5NvCnFbiP\"\n",
    "\n",
    "lastfm_api_key = \"97d5a64d5ba4a8bc580b752ceff3b87f\"\n",
    "lastfm_secret = \"35175090bd61f6f16ac607bd26e5b1de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://ws.audioscrobbler.com/2.0/'\n",
    "\n",
    "def lastfm_get(payload):\n",
    "    headers = {'user-agent': 'DataCollectorBot'}\n",
    "    payload['api_key'] = lastfm_api_key\n",
    "    payload['format'] = 'json'\n",
    "    response = requests.get(base_url, headers=headers, params=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_recent_tracks(user):\n",
    "    payload = {'method': 'user.getrecenttracks', 'user': user}\n",
    "    return lastfm_get(payload)\n",
    "\n",
    "def get_weekly_artist_chart(user):\n",
    "    payload = {'method': 'user.getweeklyartistchart', 'user': user}\n",
    "    return lastfm_get(payload)\n",
    "\n",
    "def get_weekly_track_chart(user):\n",
    "    payload = {'method': 'user.getweeklytrackchart', 'user': user}\n",
    "    return lastfm_get(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Users: 100%|██████████| 9483/9483 [2:08:50<00:00,  1.23user/s]  \n"
     ]
    }
   ],
   "source": [
    "def get_one_month_ago_timestamp():\n",
    "    one_month_ago = datetime.now() - timedelta(days=30)\n",
    "    return int(one_month_ago.timestamp())\n",
    "\n",
    "def recent_tracks_for_user_to_df(user, min_tracks=50, max_tracks=100):\n",
    "    from_timestamp = get_one_month_ago_timestamp()\n",
    "    \n",
    "    payload = {\n",
    "        'method': 'user.getrecenttracks',\n",
    "        'user': user,\n",
    "        'from': from_timestamp,\n",
    "        'limit': max_tracks \n",
    "    }\n",
    "    \n",
    "    recent_tracks = lastfm_get(payload)\n",
    "    tracks_list = []\n",
    "    \n",
    "    if 'track' in recent_tracks.get('recenttracks', {}):\n",
    "        for track in recent_tracks['recenttracks']['track']:\n",
    "            if 'date' in track: \n",
    "                track_info = {\n",
    "                    'User': user, \n",
    "                    'Artist': track['artist']['#text'],\n",
    "                    'Track Name': track['name'],\n",
    "                    'Timestamp': track['date']['uts']\n",
    "                }\n",
    "                tracks_list.append(track_info)\n",
    "\n",
    "    df = pd.DataFrame(tracks_list)\n",
    "    return df\n",
    "\n",
    "def recent_tracks_all_users_to_df(users):\n",
    "    all_tracks_dfs = [] \n",
    "    total_users = len(users)\n",
    "    \n",
    "    with tqdm(total=total_users, desc=\"Processing Users\", unit=\"user\") as pbar:\n",
    "        for user in users:\n",
    "            df = recent_tracks_for_user_to_df(user)\n",
    "            all_tracks_dfs.append(df)\n",
    "            pbar.update(1) \n",
    "    \n",
    "    combined_df = pd.concat(all_tracks_dfs, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "df = pd.read_csv('../../Downloads/user_songs_filtered.csv')\n",
    "users = df[\"Username\"].unique()\n",
    "combined_tracks_df = recent_tracks_all_users_to_df(users)\n",
    "combined_tracks_df.to_excel(\"../../Downloads/Users_Songs_Timestamps.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_df(data_list, columns):\n",
    "    if data_list:\n",
    "        df = pd.DataFrame(data_list, columns=columns)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "def get_weekly_artist_chart_df(user):\n",
    "    result = get_weekly_artist_chart(user)\n",
    "    artists = []\n",
    "    if 'weeklyartistchart' in result and 'artist' in result['weeklyartistchart']:\n",
    "        for item in result['weeklyartistchart']['artist']:\n",
    "            artists.append({\n",
    "                'Artist': item['name'],\n",
    "                'Play Count': item['playcount']\n",
    "            })\n",
    "    return list_to_df(artists, ['Artist', 'Play Count'])\n",
    "\n",
    "def get_weekly_track_chart_df(user):\n",
    "    result = get_weekly_track_chart(user)\n",
    "    tracks = []\n",
    "    if 'weeklytrackchart' in result and 'track' in result['weeklytrackchart']:\n",
    "        for item in result['weeklytrackchart']['track']:\n",
    "            tracks.append({\n",
    "                'Track Name': item['name'],\n",
    "                'Artist': item['artist']['#text'],\n",
    "                'Play Count': item['playcount']\n",
    "            })\n",
    "    return list_to_df(tracks, ['Track Name', 'Artist', 'Play Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = combined_tracks_df[:100000]\n",
    "df = pd.read_excel(\"../../Downloads/Users_Songs_Timestamps.xlsx\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "df['Time_of_Day'] = df['Timestamp'].dt.hour\n",
    "df['Artist_Track'] = df['Artist'].astype(str) + ' - ' + df['Track Name'].astype(str)\n",
    "\n",
    "time_of_day_encoded = pd.get_dummies(df['Time_of_Day'], prefix='hour')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['Artist_Track_Encoded'] = label_encoder.fit_transform(df['Artist_Track'])\n",
    "\n",
    "df = pd.concat([df, time_of_day_encoded], axis=1)\n",
    "\n",
    "sequence_length = 3\n",
    "vocab_size = len(label_encoder.classes_)\n",
    "\n",
    "\n",
    "X_seq_list, y_seq_list = [], []\n",
    "\n",
    "for _, group in df.groupby('User'):\n",
    "    group = group.sort_values('Timestamp')\n",
    "    \n",
    "    for i in range(len(group) - sequence_length + 1):\n",
    "        artist_track_sequence = group['Artist_Track_Encoded'].iloc[i:i + sequence_length - 1].values\n",
    "        \n",
    "        time_features_sequence = group[time_of_day_encoded.columns].iloc[i:i + sequence_length - 1].values.reshape((sequence_length - 1) * len(time_of_day_encoded.columns))\n",
    "        \n",
    "        sequence = np.hstack([artist_track_sequence, time_features_sequence])\n",
    "        \n",
    "        label = group['Artist_Track_Encoded'].iloc[i + sequence_length - 1]\n",
    "        \n",
    "        X_seq_list.append(sequence)\n",
    "        y_seq_list.append(label)\n",
    "\n",
    "X_seq = np.array(X_seq_list)\n",
    "# y_seq = to_categorical(y_seq_list, num_classes=vocab_size)\n",
    "y_seq = np.array(y_seq_list) # Integer instead of one hot encoding\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "num_artist_track_features = sequence_length - 1  \n",
    "num_time_features = 24 * (sequence_length - 1)  \n",
    "\n",
    "X_train_artist_track = X_train[:, :num_artist_track_features]  \n",
    "X_train_time_features = X_train[:, num_artist_track_features:] \n",
    "\n",
    "X_test_artist_track = X_test[:, :num_artist_track_features] \n",
    "X_test_time_features = X_test[:, num_artist_track_features:]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated number of time features: 48\n"
     ]
    }
   ],
   "source": [
    "num_time_features_actual = X_train_time_features.shape[1]\n",
    "\n",
    "num_time_features = num_time_features_actual\n",
    "print(\"Updated number of time features:\", num_time_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " artist_track_input (InputLayer  [(None, 2)]         0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 2, 50)        15286150    ['artist_track_input[0][0]']     \n",
      "                                                                                                  \n",
      " time_features_input (InputLaye  [(None, 48)]        0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 40)           14560       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 40)           1960        ['time_features_input[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 80)           0           ['lstm_1[0][0]',                 \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 80)           0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 80)          320         ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          8100        ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 305722)       30877922    ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,189,012\n",
      "Trainable params: 46,188,852\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense, Bidirectional, BatchNormalization, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    patience=10,  \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "artist_track_input = Input(shape=(sequence_length-1,), dtype='int32', name='artist_track_input')\n",
    "time_features_input = Input(shape=(num_time_features,), name='time_features_input')  \n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_size + 1, output_dim=50, input_length=sequence_length-1)(artist_track_input)\n",
    "lstm_layer = LSTM(40, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "\n",
    "time_dense_layer = Dense(40, activation='relu')(time_features_input)\n",
    "\n",
    "combined = concatenate([lstm_layer, time_dense_layer])\n",
    "\n",
    "x = Dropout(0.5)(combined)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[artist_track_input, time_features_input], outputs=output)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X_artist_track, X_time_features, y, batch_size):\n",
    "\n",
    "    num_samples = X_artist_track.shape[0]\n",
    "    while True: \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_X_artist_track = X_artist_track[offset:offset+batch_size]\n",
    "            batch_X_time_features = X_time_features[offset:offset+batch_size]\n",
    "            batch_y = y[offset:offset+batch_size]\n",
    "            \n",
    "            \n",
    "            yield [batch_X_artist_track, batch_X_time_features], batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/4490 [..............................] - ETA: 56:03:18 - loss: 20.8018 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "steps_per_epoch = np.ceil(X_train_artist_track.shape[0] / batch_size)\n",
    "\n",
    "train_generator = data_generator(X_train_artist_track, X_train_time_features, y_train, batch_size)\n",
    "validation_generator = data_generator(X_test_artist_track, X_test_time_features, y_test, batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=np.ceil(X_test_artist_track.shape[0] / batch_size), \n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lstm_model')\n",
    "\n",
    "# model = load_model('lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613/613 [==============================] - 348s 567ms/step\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.43 GiB for an array with shape (19594, 19594) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m y_pred_top_k \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(top_k_indices\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39mreshape(top_k_indices\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m y_true_names \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(y_true)\n\u001b[1;32m----> 9\u001b[0m binary_relevance \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true_names\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_pred_top_k\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m predicted_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(k)  \n\u001b[0;32m     12\u001b[0m all_positives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.43 GiB for an array with shape (19594, 19594) and data type int32"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_prob = model.predict([X_test_artist_track, X_test_time_features])\n",
    "y_true = y_test\n",
    "k = 30\n",
    "top_k_indices = np.argsort(y_pred_prob, axis=1)[:, -k:][:, ::-1]  \n",
    "\n",
    "y_pred_top_k = label_encoder.inverse_transform(top_k_indices.flatten()).reshape(top_k_indices.shape)\n",
    "y_true_names = label_encoder.inverse_transform(y_true)\n",
    "\n",
    "# binary_relevance = np.array([[1 if label in pred[:k] else 0 for label in y_true_names] for pred in y_pred_top_k])\n",
    "# predicted_scores = np.random.rand(k)  \n",
    "\n",
    "all_positives = len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit Rate at k\n",
    "Purpose: Measures whether the true item appears in the top-k recommendations.\n",
    "Implementation: Your approach correctly computes whether the true label is among the top k predictions for each instance and averages these outcomes to get the hit rate.\n",
    "Correctness: Yes, your implementation is correct for measuring the hit rate.\n",
    "\n",
    "Precision@30 and Recall@30\n",
    "Purpose: Precision@30 measures the proportion of relevant items among the top 30 recommendations, whereas Recall@30 measures the proportion of relevant items that were recommended in the top 30.\n",
    "Implementation: Your calculation for both precision and recall seems logically correct, considering the context of a single-label prediction task where each user-item interaction has one true label.\n",
    "Correctness: Yes, but ensure that the context of your recommendation system aligns with the assumptions made in these calculations (e.g., single-label vs. multi-label scenarios).\n",
    "\n",
    "Mean Average Precision (MAP)\n",
    "Purpose: MAP@k averages the precision scores calculated at each rank position up to k, for all users, considering only the order of the relevant items.\n",
    "Implementation: You've provided an implementation that calculates MAP over the list of true labels and the predicted rankings. Your function mapk correctly averages the AP scores across all examples.\n",
    "Correctness: Yes, the MAP calculation is correctly implemented for the recommendation system's context.\n",
    "\n",
    "NDCG@30\n",
    "Purpose: NDCG@30 evaluates the ranking quality by comparing the order of recommended items to the order of true relevance, penalizing incorrect rankings based on their positions.\n",
    "Implementation: Your approach to computing NDCG in batches is particularly commendable for handling large datasets efficiently. The calculation of DCG and IDCG seems correct, accounting for the relevance scores and the logarithmic discount.\n",
    "Correctness: Yes, provided that the true relevance scores are binary (relevant or not relevant), which seems to be the assumption in your context.\n",
    "\n",
    "Additional Considerations\n",
    "\n",
    "Data Leakage: Ensure there's no data leakage in how you split the data for training and testing, especially in time-sensitive data like music listening history. Your split method appears to respect temporal order, which is good.\n",
    "\n",
    "Shuffling Data for Train-Test Split: You've set shuffle=False in train_test_split, which is correct for time-series data to avoid future information leaking into the training set.\n",
    "Early Stopping: The use of early stopping with patience and restoring best weights is a good practice to avoid overfitting while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate at 30: 0.0469\n",
      "Precision@30: 0.0469\n",
      "Recall@30: 0.0469\n",
      "MAP: 0.0070\n"
     ]
    }
   ],
   "source": [
    "def calculate_top_30_accuracy_and_print_recommendations(y_pred_prob, y_true, label_encoder):\n",
    "    \"\"\"\n",
    "    Calculate the top-30 accuracy and print the recommendations and true values for a sample.\n",
    "    \"\"\"\n",
    "    k = 30\n",
    "    top_k_indices = np.argsort(y_pred_prob, axis=1)[:, -k:][:, ::-1]  # Corrected slicing here\n",
    "    top_k_accuracy_list = []\n",
    "\n",
    "    for i, (top_k, true) in enumerate(zip(top_k_indices, y_true)):\n",
    "        hit = true in top_k\n",
    "        top_k_accuracy_list.append(int(hit))\n",
    "\n",
    "        recommended_names = label_encoder.inverse_transform(top_k)\n",
    "        true_name = label_encoder.inverse_transform([true])[0]\n",
    "        \n",
    "        # Printing disabled for brevity; uncomment for debugging or inspection\n",
    "        # print(f\"Top-30 Recommendations: {recommended_names}\")\n",
    "        # print(f\"True Value: {true_name}\")\n",
    "        # print(\"Hit:\", hit)\n",
    "\n",
    "    top_30_accuracy = np.mean(top_k_accuracy_list)\n",
    "    print(f\"Top-30 Accuracy: {top_30_accuracy*100:.2f}%\")\n",
    "    \n",
    "    return top_30_accuracy\n",
    "\n",
    "def compute_hit_rate_at_k(y_true, y_pred_top_k, k):\n",
    "    \"\"\"\n",
    "    Computes the hit rate at k, which is a simple form of accuracy measurement\n",
    "    at k, indicating whether the true label is among the top k predictions.\n",
    "\n",
    "    :param y_true: True labels (already integer-encoded).\n",
    "    :param y_pred_top_k: The top k predictions for each sample (already integer-encoded).\n",
    "    :param k: The top k selections to consider.\n",
    "    :return: The hit rate at k.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    for true, pred in zip(y_true, y_pred_top_k):\n",
    "        # Check if the true label is within the top k predictions for this sample\n",
    "        if true in pred[:k]:\n",
    "            hits += 1\n",
    "    return hits / len(y_true)\n",
    "\n",
    "def precision_at_30(y_true, y_pred_top_k):\n",
    "    correct_predictions = sum(1 for true, pred in zip(y_true, y_pred_top_k) if true in pred[:30])\n",
    "    return correct_predictions / len(y_pred_top_k)\n",
    "\n",
    "def recall_at_30(y_true, y_pred_top_k):\n",
    "    # Assuming y_true could have multiple labels in a multi-label scenario, adjust accordingly.\n",
    "    # Here, it's assumed y_true is a list of lists (even if there's typically one label per list).\n",
    "    hits = sum(1 for true, pred in zip(y_true, y_pred_top_k) if true in pred[:30])\n",
    "    total_relevant = len(y_true)  # Adjust if y_true structure is different\n",
    "    return hits / total_relevant\n",
    "\n",
    "\n",
    "def average_precision_at_k(y_true, y_score, k=30):\n",
    "    # Assuming y_score is sorted in descending order of prediction confidence\n",
    "    # And y_true is binary (1 for relevant items, 0 for irrelevant)\n",
    "    y_true = np.asarray(y_true)[:k]\n",
    "    y_score = np.asarray(y_score)[:k]\n",
    "\n",
    "    if not y_true.any():\n",
    "        return 0\n",
    "\n",
    "    score = 0\n",
    "    num_hits = 0\n",
    "    for i, (p, rel) in enumerate(zip(y_score, y_true), 1):\n",
    "        if rel:\n",
    "            num_hits += 1\n",
    "            score += num_hits / i\n",
    "    return score / np.sum(y_true)\n",
    "\n",
    "def apk(actual, predicted, k=30):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    Parameters:\n",
    "        actual : list\n",
    "            A list of elements that are to be predicted (order doesn't matter)\n",
    "        predicted : list\n",
    "            A list of predicted elements (order does matter)\n",
    "        k : int, optional\n",
    "            The maximum number of predicted elements\n",
    "    Returns:\n",
    "        score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=30):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    Parameters:\n",
    "        actual : list\n",
    "            A list of lists of elements that are to be predicted (order doesn't matter in the lists)\n",
    "        predicted : list\n",
    "            A list of lists of predicted elements (order matters in the lists)\n",
    "        k : int, optional\n",
    "            The maximum number of predicted elements\n",
    "    Returns:\n",
    "        score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "\n",
    "\n",
    "hit_rate = compute_hit_rate_at_k(y_true_names, y_pred_top_k, k)\n",
    "print(f\"Hit Rate at {k}: {hit_rate:.4f}\")\n",
    "precision_30 = precision_at_30(y_true_names, y_pred_top_k)\n",
    "print(f\"Precision@30: {precision_30:.4f}\")\n",
    "recall_30 = recall_at_30(y_true_names, y_pred_top_k)\n",
    "print(f\"Recall@30: {recall_30:.4f}\")\n",
    "\n",
    "\n",
    "# For MAP, assuming y_true_names is a list of actual artist-track names and y_pred_top_k is a list of lists of predictions\n",
    "map_score = mapk([[y] for y in y_true_names], y_pred_top_k, k=30)\n",
    "print(f\"MAP: {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NDCG@30: 0.0128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def precompute_logarithms(k):\n",
    "    \"\"\"Pre-compute logarithms for the DCG calculation.\"\"\"\n",
    "    return np.log2(np.arange(2, k + 2))\n",
    "\n",
    "def calculate_batch_ndcg(y_true_batch, y_pred_prob_batch, k, precomputed_logs):\n",
    "    \"\"\"Calculate NDCG for a single batch.\"\"\"\n",
    "    top_k_indices = np.argpartition(y_pred_prob_batch, -k)[:, -k:]\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for true_label, indices in zip(y_true_batch, top_k_indices):\n",
    "        # Determine if the true label is in the top k predictions\n",
    "        sorted_indices = np.argsort(-y_pred_prob_batch[np.arange(len(indices)), indices])\n",
    "        is_relevant = (true_label == indices[sorted_indices]).astype(int)\n",
    "        \n",
    "        # Compute DCG using pre-computed logarithms\n",
    "        dcg = np.sum((2**is_relevant - 1) / precomputed_logs[sorted_indices])\n",
    "        idcg = np.sum((2**1 - 1) / precomputed_logs[:np.sum(is_relevant)])\n",
    "        ndcg_score = dcg / idcg if idcg > 0 else 0\n",
    "        ndcg_scores.append(ndcg_score)\n",
    "\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def calculate_ndcg_in_batches(y_true, y_pred_prob, k=30, batch_size=1000):\n",
    "    \"\"\"Calculate NDCG for all predictions in batches to conserve memory.\"\"\"\n",
    "    num_samples = y_true.shape[0]\n",
    "    precomputed_logs = precompute_logarithms(k)\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_ndcg_score = calculate_batch_ndcg(\n",
    "            y_true[start_idx:end_idx],\n",
    "            y_pred_prob[start_idx:end_idx],\n",
    "            k,\n",
    "            precomputed_logs\n",
    "        )\n",
    "        ndcg_scores.append(batch_ndcg_score)\n",
    "\n",
    "    # Calculate the mean NDCG score across all batches\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "    return mean_ndcg\n",
    "\n",
    "# Assuming y_true contains integer-encoded true labels\n",
    "# and y_pred_prob contains the prediction probabilities from your model\n",
    "mean_ndcg_score = calculate_ndcg_in_batches(y_true, y_pred_prob, k=30, batch_size=1000)\n",
    "print(f\"Mean NDCG@30: {mean_ndcg_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
