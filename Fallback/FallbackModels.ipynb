{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Text based Representation TF - IDF (We have less context, not so resrouce_intensive)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../Downloads/songs_feature_eng_pca.csv\")\n",
    "df = df[['track_name', \"artist_name\", \"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pcs = df[['PC1', \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"]]\n",
    "\n",
    "f = pcs.shape[1]\n",
    "\n",
    "t = AnnoyIndex(f, 'angular')  # 'angular' is equivalent to cosine similarity\n",
    "\n",
    "for i, vector in enumerate(pcs.to_numpy()):\n",
    "    t.add_item(i, vector)\n",
    "\n",
    "t.build(10) # Adjust more - more precise but take longer\n",
    "\n",
    "top_5_similar = {i: [] for i in range(pcs.shape[0])}\n",
    "\n",
    "for i in range(pcs.shape[0]):\n",
    "    nearest = t.get_nns_by_item(i, 6, include_distances=True)\n",
    "    \n",
    "    indices, distances = nearest[0][1:], nearest[1][1:]\n",
    "    \n",
    "    similarities = [1 - d for d in distances]\n",
    "    \n",
    "    top_5_similar[i] = list(zip(indices, similarities))\n",
    "\n",
    "for i in range(1, 6):\n",
    "    df[f'Track_Name_{i}'] = np.nan\n",
    "    df[f'Artist_Name_{i}'] = np.nan\n",
    "    df[f'Similarity_{i}'] = np.nan\n",
    "\n",
    "for idx, sims in top_5_similar.items():\n",
    "    for i, (sim_idx, sim_score) in enumerate(sims, start=1):\n",
    "        df.at[idx, f'Track_Name_{i}'] = df.at[sim_idx, 'track_name']\n",
    "        df.at[idx, f'Artist_Name_{i}'] = df.at[sim_idx, 'artist_name']\n",
    "        df.at[idx, f'Similarity_{i}'] = sim_score\n",
    "\n",
    "df.to_csv(\"../../Downloads/BT4222ProjectExcel/songs_with_similarities_final.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wyero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from lightfm.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "\n",
    "lastfm_api_key = \"97d5a64d5ba4a8bc580b752ceff3b87f\"\n",
    "lastfm_secret = \"35175090bd61f6f16ac607bd26e5b1de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 27 friends so far...\n",
      "Error fetching data for user astonbrown: no such page\n",
      "Collected 77 friends so far...\n",
      "Error fetching data for user latenightcryout: no such page\n",
      "Collected 100 friends so far...\n",
      "              User                                             Tracks  \\\n",
      "0       astonbrown  [Bring Me Your Loves, Rebound, Digital Witness...   \n",
      "1         liliwer7  [Fuck The Industry Pt. 2, Calling My Phone, Gl...   \n",
      "2  latenightcryout  [I H3ART Y0U, Jealous, Romantic Homicide, Touc...   \n",
      "3         cabnfver  [Dionysus, IDOL, Maneater, Mad World, PUMPED U...   \n",
      "4  no_eyes_no_ears  [6 Five Heartbeats (feat. Vince Staples), Free...   \n",
      "\n",
      "                                             Artists  \\\n",
      "0  [St. Vincent, Jennifer Lopez, St. Vincent, Ari...   \n",
      "1  [YoungBoy Never Broke Again, Lil Tjay, 6lack, ...   \n",
      "2  [BOY FANTASY, Eyedress, d4vd, Cigarettes After...   \n",
      "3  [BTS, BTS, Nelly Furtado, Tears for Fears, 3TE...   \n",
      "4  [The Alchemist, IceWear Vezzo, Gogetter, Veeze...   \n",
      "\n",
      "                                          Playcounts  \n",
      "0                     [6, 4, 4, 3, 3, 3, 3, 3, 2, 2]  \n",
      "1           [43, 41, 37, 36, 35, 35, 35, 34, 33, 32]  \n",
      "2  [561, 418, 375, 360, 328, 313, 306, 287, 286, ...  \n",
      "3  [194, 179, 175, 167, 143, 131, 128, 120, 120, ...  \n",
      "4           [49, 40, 37, 37, 35, 34, 33, 32, 29, 28]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from collections import deque\n",
    "def get_lastfm_friends_bfs(start_username, api_key, min_users=5000):\n",
    "    discovered = set([start_username]) \n",
    "    queue = deque([start_username])     \n",
    "    collected_friends = []              \n",
    "\n",
    "    while queue and len(collected_friends) < min_users:\n",
    "        current_user = queue.popleft()\n",
    "        url = f\"http://ws.audioscrobbler.com/2.0/?method=user.getfriends&user={current_user}&api_key={api_key}&format=json\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'error' in data:\n",
    "                print(f\"Error fetching data for user {current_user}: {data['message']}\")\n",
    "                continue\n",
    "            \n",
    "            users = data.get('friends', {}).get('user', [])\n",
    "            for user in users:\n",
    "                friend_name = user['name']\n",
    "                if friend_name not in discovered:\n",
    "                    discovered.add(friend_name)\n",
    "                    queue.append(friend_name)\n",
    "                    collected_friends.append(friend_name)\n",
    "                    if len(collected_friends) >= min_users:\n",
    "                        break  \n",
    "\n",
    "            print(f\"Collected {len(collected_friends)} friends so far...\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing user {current_user}: {e}\")\n",
    "    \n",
    "    return collected_friends[:min_users]\n",
    "\n",
    "def get_top_tracks_for_users(users, api_key):\n",
    "    user_tracks = []\n",
    "    user_artists = []\n",
    "    user_playcounts = []\n",
    "    user_ids = []\n",
    "    \n",
    "    for user in users:\n",
    "        result = get_top_tracks(user, api_key)  \n",
    "        tracks, artists, playcounts = [], [], []\n",
    "        \n",
    "        for item in result['toptracks']['track'][:10]: \n",
    "            tracks.append(item['name'])\n",
    "            artists.append(item['artist']['name'])\n",
    "            playcounts.append(item['playcount'])\n",
    "        \n",
    "        user_tracks.append(tracks)\n",
    "        user_artists.append(artists)\n",
    "        user_playcounts.append(playcounts)\n",
    "        user_ids.append(user)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'User': user_ids,\n",
    "        'Tracks': user_tracks,\n",
    "        'Artists': user_artists,\n",
    "        'Playcounts': user_playcounts\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_top_tracks(user, api_key):\n",
    "    url = f\"http://ws.audioscrobbler.com/2.0/?method=user.gettoptracks&user={user}&api_key={api_key}&format=json\"\n",
    "    response = requests.get(url)\n",
    "    result = response.json()\n",
    "    return result\n",
    "\n",
    "start_username = \"Bans77\" \n",
    "\n",
    "users = get_lastfm_friends_bfs(start_username, lastfm_api_key, min_users=100)\n",
    "\n",
    "df_top_tracks = get_top_tracks_for_users(users, lastfm_api_key)\n",
    "\n",
    "print(df_top_tracks.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i, row in df_top_tracks.iterrows():\n",
    "    user = row['User']\n",
    "    for track, artist, playcount in zip(row['Tracks'], row['Artists'], row['Playcounts']):\n",
    "        track_artist = f\"{track} - {artist}\"\n",
    "        records.append((user, track_artist, playcount))\n",
    "\n",
    "df_flat = pd.DataFrame(records, columns=['User', 'Track_Artist', 'Playcount'])\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(users=df_flat['User'].unique(),\n",
    "            items=df_flat['Track_Artist'].unique())\n",
    "\n",
    "(interactions, weights) = dataset.build_interactions([(x['User'], x['Track_Artist'], float(x['Playcount'])) for index, x in df_flat.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "df_flat['user_id_code'] = df_flat['User'].astype(\"category\").cat.codes\n",
    "df_flat['item_id_code'] = df_flat['Track_Artist'].astype(\"category\").cat.codes\n",
    "\n",
    "user_item_matrix = coo_matrix((df_flat['Playcount'].astype(np.float32),\n",
    "                                (df_flat['user_id_code'], df_flat['item_id_code'])))\n",
    "\n",
    "user_item_matrix_csr = user_item_matrix.tocsr()\n",
    "\n",
    "item_user_matrix = user_item_matrix.T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wyero\\anaconda3\\envs\\cv-hw1\\lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 16 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf2b417df1643ddbaa5c8e9def5fed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended tracks for user Bans77 using Implicit: ['How to disappear - Lana Del Rey', 'A&W - Lana Del Rey', 'Pistol - Cigarettes After Sex', 'Fuck it I love you - Lana Del Rey', 'Sweet - Lana Del Rey', 'Paris, Texas (feat. SYML) - Lana Del Rey', 'Kintsugi - Lana Del Rey', 'If You Lie Down With Me - Lana Del Rey', 'Sunset - Caroline Polachek', 'Happiness is a butterfly - Lana Del Rey']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_implicit = AlternatingLeastSquares(factors=10, iterations=5, calculate_training_loss=True)\n",
    "model_implicit.fit(user_item_matrix_csr)\n",
    "\n",
    "user_id_map = dict(zip(df_flat['User'].astype(\"category\"), df_flat['user_id_code']))\n",
    "item_id_map = dict(zip(df_flat['Track_Artist'].astype(\"category\"), df_flat['item_id_code']))\n",
    "\n",
    "user_code_to_id_map = {v: k for k, v in user_id_map.items()}\n",
    "item_code_to_id_map = {v: k for k, v in item_id_map.items()}\n",
    "\n",
    "def recommend_implicit(user_id, model, user_item_matrix_csr, user_id_map, item_code_to_id_map, n_items=10):\n",
    "\n",
    "    user_code = user_id_map.get(user_id)\n",
    "    if user_code is None:\n",
    "        raise ValueError(f\"User ID {user_id} not found.\")\n",
    "\n",
    "    recommended, _ = model.recommend(user_code, user_item_matrix_csr[user_code], N=n_items)\n",
    "\n",
    "    return [item_code_to_id_map.get(item_index, 'Unknown Item') for item_index in recommended]\n",
    "\n",
    "\n",
    "\n",
    "recommended_tracks = recommend_implicit(\"SolarSerenity\", model_implicit, user_item_matrix_csr, user_id_map, item_code_to_id_map)\n",
    "print(f\"Recommended tracks for user Bans77 using Implicit: {recommended_tracks}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
