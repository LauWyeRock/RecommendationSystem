{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wyero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Based - TF - IDF Approach (Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/IMDB_top_1000.csv\")\n",
    "\n",
    "# Observing first 5 rows\n",
    "df.head(5)\n",
    "\n",
    "# Concatenate the Title, Genre, and Description into a single string\n",
    "df['combined_features'] = df['Title'] + ' ' + df['Genre'] + ' ' + df['Description']\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_scores = tfidf.fit_transform(df['combined_features'])\n",
    "\n",
    "# Shape of tfidf vector is (number of documents, number of words)\n",
    "# TFIDF has identified 2363 words in our case\n",
    "tfidf_scores.shape\n",
    "\n",
    "#Looking at some of the IDF scores\n",
    "\n",
    "idf_values = tfidf.idf_\n",
    "\n",
    "# Get mapping from term to index\n",
    "vocab_dict = tfidf.vocabulary_\n",
    "\n",
    "# Get mapping from index to term\n",
    "reverse_vocab = {}\n",
    "for x in vocab_dict:\n",
    "  reverse_vocab[vocab_dict[x]] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Based - BERT (Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(df['combined_features'], show_progress_bar=True)\n",
    "\n",
    "\n",
    "#Shape of BERT embeddings is (number of documents, 768)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders / NumericalFeatureEmbedding / OneHotEncoder = PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We precompute the similarity matrix, because during inference we simply need\n",
    "index the matrix\n",
    "\"\"\"\n",
    "\n",
    "tfidf_cosine_sim = (tfidf_scores @ tfidf_scores.T).toarray()\n",
    "bert_cosine_sim = cosine_similarity(embeddings)\n",
    "\n",
    "#Shape of the similarity matrix (number of documents, number of documents)\n",
    "bert_cosine_sim.shape, tfidf_cosine_sim.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(data, movie_name, similarity_matrix, top_k=10):\n",
    "  #Get the index of the movie from our database\n",
    "  index = data.index[data['Title'] == movie_name][0]\n",
    "\n",
    "  # Get the similarity scores of our movie with every other movie in the database\n",
    "  score_arr = similarity_matrix[index]\n",
    "\n",
    "  # We sort the score and reverse it to get the highest correlated movie\n",
    "  # We do argsort here because we are interested in the indices.\n",
    "  reveresed_score_arr = np.argsort(score_arr)[::-1]\n",
    "\n",
    "  # Retrieve top K movies. We ignore the 0th element, because that contains the similarity score of our movie with itself\n",
    "  top_k_movies = reveresed_score_arr[1:top_k + 1]\n",
    "\n",
    "  return data.iloc[top_k_movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_movies(df, \"The Dark Knight Rises\", tfidf_cosine_sim, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_movies(df, \"The Dark Knight Rises\", bert_cosine_sim, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "# Imaginary user data\n",
    "users = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'age': [25, 30, 22, 35, 28],\n",
    "    'country': ['US', 'UK', 'US', 'DE', 'CA']\n",
    "})\n",
    "\n",
    "# Imaginary song data\n",
    "songs = pd.DataFrame({\n",
    "    'song_id': [1, 2, 3, 4, 5],\n",
    "    'title': ['Song A', 'Song B', 'Song C', 'Song D', 'Song E'],\n",
    "    'artist': ['Artist 1', 'Artist 2', 'Artist 3', 'Artist 4', 'Artist 5'],\n",
    "    'genre': ['Pop', 'Rock', 'Jazz', 'Pop', 'Classical']\n",
    "})\n",
    "\n",
    "# Imaginary interaction data\n",
    "interactions = pd.DataFrame({\n",
    "    'user_id': [1, 2, 1, 3, 4, 5, 5],\n",
    "    'song_id': [1, 2, 3, 4, 2, 1, 5],\n",
    "    'listen_count': [5, 2, 3, 1, 4, 6, 2]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset object\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users['user_id'],\n",
    "    songs['song_id'],\n",
    "    user_features=['age', 'country'],\n",
    "    item_features=['title', 'artist', 'genre']\n",
    ")\n",
    "\n",
    "# Build the interaction matrix\n",
    "(interactions_matrix, weights_matrix) = dataset.build_interactions(\n",
    "    [(x[0], x[1], x[2]) for x in interactions.values]\n",
    ")\n",
    "\n",
    "# Build user and item features\n",
    "user_features = dataset.build_user_features(\n",
    "    (x[0], {'age': x[1], 'country': x[2]}) for x in users.values\n",
    ")\n",
    "item_features = dataset.build_item_features(\n",
    "    (x[0], {'title': x[1], 'artist': x[2], 'genre': x[3]}) for x in songs.values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "# Initialize the model\n",
    "model = LightFM(loss='warp')\n",
    "\n",
    "# Train the model\n",
    "model.fit(interactions_matrix, user_features=user_features, item_features=item_features, sample_weight=weights_matrix, epochs=30)\n",
    "\n",
    "# Evaluate the model\n",
    "train_precision = precision_at_k(model, interactions_matrix, user_features=user_features, item_features=item_features).mean()\n",
    "\n",
    "print(f'Train Precision: {train_precision}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means Clustering -> PCA / tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example user data\n",
    "users = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'age': [25, 30, 22, 35, 28],\n",
    "    'country': ['US', 'UK', 'US', 'DE', 'CA']\n",
    "})\n",
    "\n",
    "# Example song data\n",
    "songs = pd.DataFrame({\n",
    "    'song_id': [1, 2, 3, 4, 5],\n",
    "    'title': ['Song A', 'Song B', 'Song C', 'Song D', 'Song E'],\n",
    "    'genre': ['Pop', 'Rock', 'Jazz', 'Pop', 'Classical'],\n",
    "    'listen_count': [100, 200, 150, 250, 300]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# One-hot encode string columns\n",
    "ohe = OneHotEncoder()\n",
    "encoded_users = ohe.fit_transform(users[['country']]).toarray()\n",
    "encoded_songs = ohe.fit_transform(songs[['title', 'genre']]).toarray()\n",
    "\n",
    "# Scale numerical columns\n",
    "scaler = StandardScaler()\n",
    "scaled_user_ages = scaler.fit_transform(users[['age']])\n",
    "scaled_song_listen_counts = scaler.fit_transform(songs[['listen_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = np.hstack((scaled_user_ages, encoded_users))\n",
    "song_features = np.hstack((scaled_song_listen_counts, encoded_songs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)  # Adjust n_components based on your needs\n",
    "pca_user_features = pca.fit_transform(user_features)\n",
    "pca_song_features = pca.fit_transform(song_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "tsne_user_features = tsne.fit_transform(pca_user_features)\n",
    "tsne_song_features = tsne.fit_transform(pca_song_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Adjust n_clusters based on your needs\n",
    "kmeans_users = KMeans(n_clusters=3).fit(tsne_user_features)\n",
    "kmeans_songs = KMeans(n_clusters=3).fit(tsne_song_features)\n",
    "\n",
    "# Assign clusters back to the dataframes\n",
    "users['cluster'] = kmeans_users.labels_\n",
    "songs['cluster'] = kmeans_songs.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# New song features (after applying the same preprocessing, PCA, and t-SNE as before)\n",
    "new_song_features = [...]  # This should be a 2D array after dimensionality reduction\n",
    "\n",
    "# Find the closest cluster for the new song\n",
    "closest_cluster, _ = pairwise_distances_argmin_min(new_song_features, kmeans_songs.cluster_centers_)\n",
    "\n",
    "print(f\"The new song belongs to cluster: {closest_cluster[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_for_new_user(new_user_features, songs, kmeans_users, kmeans_songs):\n",
    "    # Assuming new_user_features is preprocessed, PCA, and t-SNE applied\n",
    "    # Assign the new user to a cluster\n",
    "    closest_cluster, _ = pairwise_distances_argmin_min(new_user_features, kmeans_users.cluster_centers_)\n",
    "    user_cluster = closest_cluster[0]\n",
    "    \n",
    "    # Find songs in the same cluster\n",
    "    recommended_songs = songs[songs['cluster'] == user_cluster]\n",
    "    \n",
    "    return recommended_songs\n",
    "\n",
    "# Example usage\n",
    "new_user_features = [...]  # Preprocessed features of the new user\n",
    "recommended_songs = recommend_songs_for_new_user(new_user_features, songs, kmeans_users, kmeans_songs)\n",
    "print(recommended_songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
