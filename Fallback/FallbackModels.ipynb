{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wyero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Text based Representation TF - IDF (We have less context, not so resrouce_intensive)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def parse_array(s):\n",
    "    try:\n",
    "        if pd.isna(s) or not isinstance(s, str):\n",
    "            return np.array([])\n",
    "        numbers = s.strip(\"[]\").split()\n",
    "        return np.array([float(num) for num in numbers])\n",
    "    except ValueError:\n",
    "        return np.array([])\n",
    "\n",
    "df = pd.read_excel(\"../../Downloads/ReRun7.xlsx\")\n",
    "df = df[['Song', \"Artist\", \"featured_artists\", \"duration\", \"album\", \"mfcc\", 'chroma', 'rms', 'spectral_centroid', 'zcr', 'tempo']]\n",
    "df = df.dropna(subset=['mfcc', 'chroma', 'rms', 'spectral_centroid', 'zcr', 'tempo'])\n",
    "df['combined_text_features'] = df['Song'] + ' ' + df['Artist'] + \" \" + df['featured_artists'] + ' ' + df['album']\n",
    "# Determine the size of non-text features by looking at the first non-empty row\n",
    "# This assumes that all non-empty rows have features of the same size\n",
    "non_text_features = ['mfcc', 'chroma', 'rms', 'spectral_centroid', 'zcr', 'tempo']\n",
    "feature_sizes = {}\n",
    "for feature in non_text_features:\n",
    "    df[feature] = df[feature].apply(parse_array)\n",
    "    \n",
    "    for array in df[feature]:\n",
    "        if array.size > 0:  \n",
    "            feature_sizes[feature] = array.size\n",
    "            break  \n",
    "\n",
    "for feature, size in feature_sizes.items():\n",
    "    df[feature] = df[feature].apply(lambda x: np.pad(x, (0, max(0, size - x.size)), mode='constant')[:size] if x.size > 0 else np.zeros(size))\n",
    "\n",
    "df['combined_non_text_features'] = df.apply(lambda row: np.concatenate([row[feat] for feat in non_text_features]), axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_non_text_features = scaler.fit_transform(np.stack(df['combined_non_text_features'].values))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text_features'].values.astype('U'))\n",
    "\n",
    "scaled_non_text_features_sparse = csr_matrix(scaled_non_text_features)\n",
    "combined_features = hstack([tfidf_matrix, scaled_non_text_features_sparse])\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000) \n",
    "reduced_features = svd.fit_transform(combined_features)\n",
    "cosine_sim_matrix = cosine_similarity(reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Song          Artist  Similarity\n",
      "44870              You're Beautiful     James Blunt    0.805199\n",
      "8324   Wicked Game - Acoustic; Live      Stone Sour    0.758321\n",
      "22668                     Sometimes  Britney Spears    0.753746\n",
      "30463                        Circle        Greywind    0.744073\n",
      "15597           Another Man's Woman      Supertramp    0.741546\n"
     ]
    }
   ],
   "source": [
    "def recommend_song(song_name, data, similarity_matrix, top_k=10):\n",
    "\n",
    "    normalized_song_names = data['Song'].str.lower().str.strip()\n",
    "    song_name_normalized = song_name.lower().strip()\n",
    "    \n",
    "    if song_name_normalized not in normalized_song_names.values:\n",
    "        print(f\"Song named '{song_name}' does not exist in the dataset.\")\n",
    "        return None\n",
    "    \n",
    "    song_idx = data.index[data['Song'] == song_name].tolist()[0]\n",
    "    \n",
    "    song_similarities = similarity_matrix[song_idx]\n",
    "    similar_idxs = np.argsort(song_similarities)[::-1]\n",
    "    \n",
    "    top_k_idxs = similar_idxs[1:top_k + 1]\n",
    "    \n",
    "    top_similar_songs = data.iloc[top_k_idxs].copy()\n",
    "    top_similar_songs['Similarity'] = song_similarities[top_k_idxs]\n",
    "    \n",
    "    return top_similar_songs\n",
    "\n",
    "song_name = \"I Don't Care\"  \n",
    "similar_songs = recommend_song(song_name, df, cosine_sim_matrix, top_k=5)\n",
    "print(similar_songs[['Song', 'Artist', 'Similarity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wyero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from lightfm.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "\n",
    "lastfm_api_key = \"97d5a64d5ba4a8bc580b752ceff3b87f\"\n",
    "lastfm_secret = \"35175090bd61f6f16ac607bd26e5b1de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 27 friends so far...\n",
      "Collected 33 friends so far...\n",
      "Collected 83 friends so far...\n",
      "Error fetching data for user latenightcryout: no such page\n",
      "Collected 100 friends so far...\n",
      "              User                                             Tracks  \\\n",
      "0       astonbrown  [Bring Me Your Loves, Rebound, Digital Witness...   \n",
      "1         liliwer7  [Fuck The Industry Pt. 2, Calling My Phone, Gl...   \n",
      "2  latenightcryout  [I H3ART Y0U, Jealous, Romantic Homicide, Touc...   \n",
      "3         cabnfver  [Dionysus, IDOL, Maneater, Mad World, PUMPED U...   \n",
      "4  no_eyes_no_ears  [6 Five Heartbeats (feat. Vince Staples), Free...   \n",
      "\n",
      "                                             Artists  \\\n",
      "0  [St. Vincent, Jennifer Lopez, St. Vincent, Ari...   \n",
      "1  [YoungBoy Never Broke Again, Lil Tjay, 6lack, ...   \n",
      "2  [BOY FANTASY, Eyedress, d4vd, Cigarettes After...   \n",
      "3  [BTS, BTS, Nelly Furtado, Tears for Fears, 3TE...   \n",
      "4  [The Alchemist, IceWear Vezzo, Gogetter, Veeze...   \n",
      "\n",
      "                                          Playcounts  \n",
      "0                     [5, 4, 4, 3, 3, 3, 3, 3, 2, 2]  \n",
      "1           [43, 41, 37, 36, 35, 35, 34, 34, 33, 32]  \n",
      "2  [561, 418, 375, 359, 328, 313, 305, 287, 286, ...  \n",
      "3  [194, 179, 175, 167, 143, 131, 128, 120, 120, ...  \n",
      "4           [49, 39, 37, 35, 35, 34, 33, 32, 29, 28]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from collections import deque\n",
    "def get_lastfm_friends_bfs(start_username, api_key, min_users=5000):\n",
    "    discovered = set([start_username]) \n",
    "    queue = deque([start_username])     \n",
    "    collected_friends = []              \n",
    "\n",
    "    while queue and len(collected_friends) < min_users:\n",
    "        current_user = queue.popleft()\n",
    "        url = f\"http://ws.audioscrobbler.com/2.0/?method=user.getfriends&user={current_user}&api_key={api_key}&format=json\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'error' in data:\n",
    "                print(f\"Error fetching data for user {current_user}: {data['message']}\")\n",
    "                continue\n",
    "            \n",
    "            users = data.get('friends', {}).get('user', [])\n",
    "            for user in users:\n",
    "                friend_name = user['name']\n",
    "                if friend_name not in discovered:\n",
    "                    discovered.add(friend_name)\n",
    "                    queue.append(friend_name)\n",
    "                    collected_friends.append(friend_name)\n",
    "                    if len(collected_friends) >= min_users:\n",
    "                        break  \n",
    "\n",
    "            print(f\"Collected {len(collected_friends)} friends so far...\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing user {current_user}: {e}\")\n",
    "    \n",
    "    return collected_friends[:min_users]\n",
    "\n",
    "def get_top_tracks_for_users(users, api_key):\n",
    "    user_tracks = []\n",
    "    user_artists = []\n",
    "    user_playcounts = []\n",
    "    user_ids = []\n",
    "    \n",
    "    for user in users:\n",
    "        result = get_top_tracks(user, api_key)  \n",
    "        tracks, artists, playcounts = [], [], []\n",
    "        \n",
    "        for item in result['toptracks']['track'][:10]: \n",
    "            tracks.append(item['name'])\n",
    "            artists.append(item['artist']['name'])\n",
    "            playcounts.append(item['playcount'])\n",
    "        \n",
    "        user_tracks.append(tracks)\n",
    "        user_artists.append(artists)\n",
    "        user_playcounts.append(playcounts)\n",
    "        user_ids.append(user)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'User': user_ids,\n",
    "        'Tracks': user_tracks,\n",
    "        'Artists': user_artists,\n",
    "        'Playcounts': user_playcounts\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_top_tracks(user, api_key):\n",
    "    url = f\"http://ws.audioscrobbler.com/2.0/?method=user.gettoptracks&user={user}&api_key={api_key}&format=json\"\n",
    "    response = requests.get(url)\n",
    "    result = response.json()\n",
    "    return result\n",
    "\n",
    "start_username = \"Bans77\" \n",
    "\n",
    "users = get_lastfm_friends_bfs(start_username, lastfm_api_key, min_users=100)\n",
    "\n",
    "df_top_tracks = get_top_tracks_for_users(users, lastfm_api_key)\n",
    "\n",
    "print(df_top_tracks.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i, row in df_top_tracks.iterrows():\n",
    "    user = row['User']\n",
    "    for track, artist, playcount in zip(row['Tracks'], row['Artists'], row['Playcounts']):\n",
    "        track_artist = f\"{track} - {artist}\"\n",
    "        records.append((user, track_artist, playcount))\n",
    "\n",
    "df_flat = pd.DataFrame(records, columns=['User', 'Track_Artist', 'Playcount'])\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(users=df_flat['User'].unique(),\n",
    "            items=df_flat['Track_Artist'].unique())\n",
    "\n",
    "(interactions, weights) = dataset.build_interactions([(x['User'], x['Track_Artist'], float(x['Playcount'])) for index, x in df_flat.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "model = LightFM(no_components=10, loss='warp')\n",
    "\n",
    "model.fit(interactions, sample_weight=weights, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     17\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBans77\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with an actual user ID from your dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m recommended_tracks \u001b[38;5;241m=\u001b[39m recommend(user_id, \u001b[43mmodel\u001b[49m, dataset, interactions, n_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended tracks for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommended_tracks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def recommend(user_id, model, data, interactions, n_items=10):\n",
    "    user_index = data.mapping()[0][user_id]\n",
    "    \n",
    "    scores = model.predict(user_index, np.arange(interactions.shape[1]))\n",
    "    \n",
    "    item_indices = np.argsort(-scores)[:n_items]  \n",
    "    \n",
    "    # Convert item indices back to item IDs\n",
    "    item_ids = [list(data.mapping()[2].keys())[i] for i in item_indices]\n",
    "    \n",
    "    return item_ids\n",
    "\n",
    "user_id = 'Bans77'  \n",
    "recommended_tracks = recommend(user_id, model, dataset, interactions, n_items=10)\n",
    "print(f\"Recommended tracks for user {user_id}: {recommended_tracks}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MANIAC - Stray Kids', 'Charmer - Stray Kids', 'VENOM - Stray Kids', 'New Tank - Playboi Carti', 'FREEZE - Stray Kids', 'Lonely St. - Stray Kids', 'Muddy Water (Changbin, Hyunjin, HAN, Felix) - Stray Kids', \"[EN-TER key] ENHYPEN's Imaginarium - ENHYPEN (엔하이픈) (ENG/JPN) - ENHYPEN\", 'Waiting For Us (Bang Chan, Lee Know, Seungmin, I.N) - Stray Kids', 'Tinnitus (Wanna Be a Rock) - TOMORROW X TOGETHER']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wyero\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pivot_table = df_flat.pivot_table(index='User', columns='Track_Artist', values='Playcount', fill_value=0)\n",
    "user_item_matrix = csr_matrix(pivot_table.values)  \n",
    "\n",
    "user_item_matrix_normalized = normalize(user_item_matrix, axis=1)\n",
    "\n",
    "k = 5  \n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "user_clusters = kmeans.fit_predict(user_item_matrix_normalized)\n",
    "\n",
    "pivot_table['Cluster'] = user_clusters\n",
    "\n",
    "def recommend_songs_for_user(user_id, pivot_table):\n",
    "    user_cluster = pivot_table.loc[user_id, 'Cluster']\n",
    "    cluster_table = pivot_table[pivot_table['Cluster'] == user_cluster]\n",
    "    \n",
    "    song_popularity = cluster_table.drop('Cluster', axis=1).mean().sort_values(ascending=False)\n",
    "    top_recommendations = song_popularity.head(10).index.tolist()\n",
    "    return top_recommendations\n",
    "\n",
    "\n",
    "recommendations = recommend_songs_for_user('astonbrown', pivot_table)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check user's original songs\n",
    "df_flat[df_flat['User'] == 'astonbrown'][['Track_Artist', 'Playcount']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
